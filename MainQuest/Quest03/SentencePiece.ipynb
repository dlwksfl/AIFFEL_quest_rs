{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë„¤ì´ë²„ ì˜í™”ë¦¬ë·° ê°ì • ë¶„ì„ ë¬¸ì œì— SentencePiece ì ìš©í•´ ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: (149995, 3)\n",
      "Test Data: (49997, 3)\n"
     ]
    }
   ],
   "source": [
    "# ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•˜ëŠ” ê³¼ì •\n",
    "train_txt = '/Users/jian_lee/Desktop/aiffel/data/text_process/Data/ratings_train.txt'\n",
    "test_txt = '/Users/jian_lee/Desktop/aiffel/data/text_process/Data/ratings_test.txt'\n",
    "\n",
    "def load_data(file):\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "    - ì²« ë²ˆì§¸ ì»¬ëŸ¼ì€ id, ë‘ ë²ˆì§¸ ì»¬ëŸ¼ì€ ë¦¬ë·° ë‚´ìš©(document), ì„¸ ë²ˆì§¸ ì»¬ëŸ¼ì€ ë¼ë²¨(label)ì´ë‹¤.\n",
    "    - ì²« ì¤„ì€ í—¤ë”ì´ë¯€ë¡œ ì œê±°í•˜ê³ , id ì»¬ëŸ¼ì€ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.\n",
    "    - ê²°ì¸¡ì¹˜ë¥¼ ì œê±°í•˜ê³  ë°ì´í„°ë¥¼ ëœë¤í•˜ê²Œ ì„ëŠ”ë‹¤.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file, delimiter='\\t', names=['id', 'document', 'label'], skiprows=1)\n",
    "    df.dropna(inplace=True)  # ê²°ì¸¡ì¹˜ ì œê±°\n",
    "    df = df.sample(frac=1).reset_index(drop=True)  # ë°ì´í„° ëœë¤ ì„ê¸°\n",
    "    return df\n",
    "\n",
    "# í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
    "train_data = load_data(train_txt)\n",
    "test_data = load_data(test_txt)\n",
    "\n",
    "print(f\"Train Data: {train_data.shape}\")\n",
    "print(f\"Test Data: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í† í¬ë‚˜ì´ì € í´ë˜ìŠ¤ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 SentencePiece í† í¬ë‚˜ì´ì € í´ë˜ìŠ¤\n",
    "class SentencePieceTokenizer:\n",
    "    def __init__(self, vocab_size=8000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sp_model_prefix = 'spm_review'\n",
    "        self.sp_model_path = f'{self.sp_model_prefix}.model'\n",
    "        self.sp = None\n",
    "        self.vocab_size_actual = None\n",
    "        self.tokenization_time = 0\n",
    "    \n",
    "    def train(self, texts):\n",
    "        \"\"\"SentencePiece ëª¨ë¸ í•™ìŠµ\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # í›ˆë ¨ ë°ì´í„° ì¤€ë¹„\n",
    "        with open('spm_input.txt', 'w', encoding='utf-8') as f:\n",
    "            for text in texts:\n",
    "                f.write(text + '\\n')\n",
    "        \n",
    "        # ëª¨ë¸ í›ˆë ¨\n",
    "        spm.SentencePieceTrainer.Train(\n",
    "            input='spm_input.txt',\n",
    "            model_prefix=self.sp_model_prefix,\n",
    "            vocab_size=self.vocab_size,\n",
    "            model_type='bpe',\n",
    "            character_coverage=0.9995,\n",
    "        )\n",
    "        \n",
    "        # ëª¨ë¸ ë¡œë“œ\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.Load(self.sp_model_path)\n",
    "        self.vocab_size_actual = self.sp.GetPieceSize()\n",
    "        \n",
    "        self.tokenization_time = time.time() - start_time\n",
    "        print(f\"SentencePiece ëª¨ë¸ í•™ìŠµ ì™„ë£Œ (ì‹œê°„: {self.tokenization_time:.2f}ì´ˆ)\")\n",
    "    \n",
    "    def load(self):\n",
    "        \"\"\"í•™ìŠµëœ SentencePiece ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "        if os.path.exists(self.sp_model_path):\n",
    "            self.sp = spm.SentencePieceProcessor()\n",
    "            self.sp.Load(self.sp_model_path)\n",
    "            self.vocab_size_actual = self.sp.GetPieceSize()\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def tokenize(self, texts):\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ í† í°í™”\"\"\"\n",
    "        start_time = time.time()\n",
    "        tokenized = [self.sp.EncodeAsIds(text) for text in texts]\n",
    "        self.tokenization_time = time.time() - start_time\n",
    "        return tokenized\n",
    "    \n",
    "    def detokenize(self, token_ids):\n",
    "        \"\"\"í† í° ID ë¦¬ìŠ¤íŠ¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\"\"\"\n",
    "        if isinstance(token_ids[0], list):\n",
    "            return [self.sp.DecodeIds(ids) for ids in token_ids]\n",
    "        return self.sp.DecodeIds(token_ids)\n",
    "\n",
    "# 2.2 KoNLPy Okt í† í¬ë‚˜ì´ì € í´ë˜ìŠ¤\n",
    "class OktTokenizer:\n",
    "    def __init__(self):\n",
    "        self.okt = Okt()\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.vocab_size = 0\n",
    "        self.tokenization_time = 0\n",
    "    \n",
    "    def train(self, texts, min_freq=2):\n",
    "        \"\"\"ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Okt í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”\n",
    "        self.okt = Okt()\n",
    "        \n",
    "        # ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "        word_counts = {}\n",
    "        for text in texts:\n",
    "            tokens = self.okt.morphs(text, stem=True)  # í˜•íƒœì†Œ ë¶„ì„ (ìŠ¤í…Œë° ì ìš©)\n",
    "            for token in tokens:\n",
    "                if token not in word_counts:\n",
    "                    word_counts[token] = 0\n",
    "                word_counts[token] += 1\n",
    "        \n",
    "        # ìµœì†Œ ë¹ˆë„ìˆ˜ ì´ìƒ ë‹¨ì–´ë§Œ ì‚¬ì „ì— ì¶”ê°€\n",
    "        vocab = ['<PAD>', '<UNK>']  # íŒ¨ë”©ê³¼ ì•Œ ìˆ˜ ì—†ëŠ” ë‹¨ì–´ë¥¼ ìœ„í•œ íŠ¹ìˆ˜ í† í°\n",
    "        vocab.extend([word for word, count in word_counts.items() if count >= min_freq])\n",
    "        \n",
    "        # ë‹¨ì–´-ì¸ë±ìŠ¤ ë§¤í•‘ êµ¬ì¶•\n",
    "        self.word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "        self.index_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "        self.vocab_size = len(vocab)\n",
    "        \n",
    "        self.tokenization_time = time.time() - start_time\n",
    "        print(f\"Okt ì–´íœ˜ ì‚¬ì „ êµ¬ì¶• ì™„ë£Œ: {self.vocab_size} ë‹¨ì–´ (ì‹œê°„: {self.tokenization_time:.2f}ì´ˆ)\")\n",
    "    \n",
    "    def tokenize(self, texts):\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ì—¬ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\"\"\"\n",
    "        start_time = time.time()\n",
    "        tokenized = []\n",
    "        for text in texts:\n",
    "            tokens = self.okt.morphs(text, stem=True)\n",
    "            indices = [self.word_to_index.get(token, 1) for token in tokens]  # 1ì€ <UNK> í† í°\n",
    "            tokenized.append(indices)\n",
    "        \n",
    "        self.tokenization_time = time.time() - start_time\n",
    "        return tokenized\n",
    "    \n",
    "    def detokenize(self, token_ids):\n",
    "        \"\"\"í† í° ID ë¦¬ìŠ¤íŠ¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\"\"\"\n",
    "        if isinstance(token_ids[0], list):\n",
    "            texts = []\n",
    "            for ids in token_ids:\n",
    "                tokens = [self.index_to_word.get(idx, '<UNK>') for idx in ids if idx != 0]  # 0ì€ <PAD> í† í°\n",
    "                texts.append(' '.join(tokens))\n",
    "            return texts\n",
    "        else:\n",
    "            tokens = [self.index_to_word.get(idx, '<UNK>') for idx in token_ids if idx != 0]\n",
    "            return ' '.join(tokens)\n",
    "\n",
    "# 2.3 KoNLPy Kkma í† í¬ë‚˜ì´ì € í´ë˜ìŠ¤\n",
    "class KkmaTokenizer:\n",
    "    def __init__(self):\n",
    "        from konlpy.tag import Kkma\n",
    "        self.kkma = Kkma()\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.vocab_size = 0\n",
    "        self.tokenization_time = 0\n",
    "    \n",
    "    def train(self, texts, min_freq=2):\n",
    "        \"\"\"ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Kkma í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”\n",
    "        from konlpy.tag import Kkma\n",
    "        self.kkma = Kkma()\n",
    "        \n",
    "        # ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "        word_counts = {}\n",
    "        for text in texts:\n",
    "            tokens = self.kkma.morphs(text)  # í˜•íƒœì†Œ ë¶„ì„\n",
    "            for token in tokens:\n",
    "                if token not in word_counts:\n",
    "                    word_counts[token] = 0\n",
    "                word_counts[token] += 1\n",
    "        \n",
    "        # ìµœì†Œ ë¹ˆë„ìˆ˜ ì´ìƒ ë‹¨ì–´ë§Œ ì‚¬ì „ì— ì¶”ê°€\n",
    "        vocab = ['<PAD>', '<UNK>']  # íŒ¨ë”©ê³¼ ì•Œ ìˆ˜ ì—†ëŠ” ë‹¨ì–´ë¥¼ ìœ„í•œ íŠ¹ìˆ˜ í† í°\n",
    "        vocab.extend([word for word, count in word_counts.items() if count >= min_freq])\n",
    "        \n",
    "        # ë‹¨ì–´-ì¸ë±ìŠ¤ ë§¤í•‘ êµ¬ì¶•\n",
    "        self.word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "        self.index_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "        self.vocab_size = len(vocab)\n",
    "        \n",
    "        self.tokenization_time = time.time() - start_time\n",
    "        print(f\"Kkma ì–´íœ˜ ì‚¬ì „ êµ¬ì¶• ì™„ë£Œ: {self.vocab_size} ë‹¨ì–´ (ì‹œê°„: {self.tokenization_time:.2f}ì´ˆ)\")\n",
    "    \n",
    "    def tokenize(self, texts):\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ì—¬ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\"\"\"\n",
    "        start_time = time.time()\n",
    "        tokenized = []\n",
    "        for text in texts:\n",
    "            tokens = self.kkma.morphs(text)\n",
    "            indices = [self.word_to_index.get(token, 1) for token in tokens]  # 1ì€ <UNK> í† í°\n",
    "            tokenized.append(indices)\n",
    "        \n",
    "        self.tokenization_time = time.time() - start_time\n",
    "        return tokenized\n",
    "    \n",
    "    def detokenize(self, token_ids):\n",
    "        \"\"\"í† í° ID ë¦¬ìŠ¤íŠ¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\"\"\"\n",
    "        if isinstance(token_ids[0], list):\n",
    "            texts = []\n",
    "            for ids in token_ids:\n",
    "                tokens = [self.index_to_word.get(idx, '<UNK>') for idx in ids if idx != 0]  # 0ì€ <PAD> í† í°\n",
    "                texts.append(' '.join(tokens))\n",
    "            return texts\n",
    "        else:\n",
    "            tokens = [self.index_to_word.get(idx, '<UNK>') for idx in token_ids if idx != 0]\n",
    "            return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ê°ì • ë¶„ì„ ëª¨ë¸ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout_rate=0.5):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=n_layers, \n",
    "            bidirectional=True, \n",
    "            dropout=dropout_rate if n_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        packed_output, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        hidden = self.dropout(hidden)\n",
    "        output = self.fc(hidden)\n",
    "        return self.sigmoid(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
    "    \"\"\"í•œ ì—í­ ë™ì•ˆ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for texts, labels in data_loader:\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.float().unsqueeze(1).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(texts)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        predicted_labels = (predictions >= 0.5).int()\n",
    "        correct_predictions += (predicted_labels == labels.int()).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    \"\"\"ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_loader:\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            \n",
    "            predictions = model(texts)\n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predicted_labels = (predictions >= 0.5).int()\n",
    "            correct_predictions += (predicted_labels == labels.int()).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ì‹¤í—˜ ì‹¤í–‰ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(tokenizer_name, tokenizer, train_data, test_data, max_len=100, batch_size=64, epochs=5):\n",
    "    \"\"\"íŠ¹ì • í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•œ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ì‹¤í—˜: {tokenizer_name} í† í¬ë‚˜ì´ì € ì‚¬ìš©\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # 1. í† í¬ë‚˜ì´ì € í›ˆë ¨ ë˜ëŠ” ë¡œë“œ\n",
    "    if tokenizer_name == 'SentencePiece':\n",
    "        if not tokenizer.load():\n",
    "            print(\"SentencePiece ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
    "            tokenizer.train(train_data['document'])\n",
    "    else:\n",
    "        print(f\"{tokenizer_name} í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì‹œì‘...\")\n",
    "        tokenizer.train(train_data['document'])\n",
    "    \n",
    "    # 2. ë°ì´í„° í† í°í™”\n",
    "    print(f\"{tokenizer_name} í† í°í™” ì‹œì‘...\")\n",
    "    train_texts = tokenizer.tokenize(train_data['document'])\n",
    "    test_texts = tokenizer.tokenize(test_data['document'])\n",
    "    \n",
    "    # í† í°í™” ì†Œìš” ì‹œê°„ ê¸°ë¡\n",
    "    print(f\"í† í°í™” ì†Œìš” ì‹œê°„: {tokenizer.tokenization_time:.2f}ì´ˆ\")\n",
    "    \n",
    "    # 3. íŒ¨ë”© ì²˜ë¦¬\n",
    "    train_texts = pad_sequences(train_texts, maxlen=max_len, padding='post')\n",
    "    test_texts = pad_sequences(test_texts, maxlen=max_len, padding='post')\n",
    "    \n",
    "    # 4. PyTorch ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„±\n",
    "    train_texts_tensor = torch.tensor(train_texts, dtype=torch.long)\n",
    "    train_labels_tensor = torch.tensor(train_data['label'].values, dtype=torch.long)\n",
    "    test_texts_tensor = torch.tensor(test_texts, dtype=torch.long)\n",
    "    test_labels_tensor = torch.tensor(test_data['label'].values, dtype=torch.long)\n",
    "    \n",
    "    # ê²€ì¦ ë°ì´í„° ë¶„ë¦¬\n",
    "    train_size = int(0.8 * len(train_texts_tensor))\n",
    "    indices = torch.randperm(len(train_texts_tensor))\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "    \n",
    "    train_dataset = TensorDataset(\n",
    "        train_texts_tensor[train_indices], \n",
    "        train_labels_tensor[train_indices]\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        train_texts_tensor[val_indices], \n",
    "        train_labels_tensor[val_indices]\n",
    "    )\n",
    "    test_dataset = TensorDataset(test_texts_tensor, test_labels_tensor)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # 5. ëª¨ë¸ ì„¤ì •\n",
    "    vocab_size = tokenizer.vocab_size_actual if hasattr(tokenizer, 'vocab_size_actual') else tokenizer.vocab_size\n",
    "    vocab_size = vocab_size + 1 if tokenizer_name == 'SentencePiece' else vocab_size  # SentencePieceëŠ” <unk> í† í° í¬í•¨\n",
    "    \n",
    "    embedding_dim = 300\n",
    "    hidden_dim = 256\n",
    "    n_layers = 2\n",
    "    dropout_rate = 0.5\n",
    "    \n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() \n",
    "                      else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    model = SentimentLSTM(vocab_size, embedding_dim, hidden_dim, n_layers, dropout_rate).to(device)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # 6. í•™ìŠµ ì‹¤í–‰\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = f'best_sentiment_model_{tokenizer_name}.pt'\n",
    "    \n",
    "    print(f\"{tokenizer_name} ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        \n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Epoch {epoch+1}: Best model saved with validation loss: {val_loss:.4f}\")\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "        print(f\"\\tVal Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"í•™ìŠµ ì†Œìš” ì‹œê°„: {training_time:.2f}ì´ˆ\")\n",
    "    \n",
    "    # 7. ìµœì  ëª¨ë¸ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    print(f\"í…ŒìŠ¤íŠ¸ ê²°ê³¼: Loss: {test_loss:.4f} | Acc: {test_acc*100:.2f}%\")\n",
    "    \n",
    "    # 8. ì‹œê°í™” ë°ì´í„° ë° ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°˜í™˜\n",
    "    metrics = {\n",
    "        'name': tokenizer_name,\n",
    "        'vocab_size': vocab_size,\n",
    "        'tokenization_time': tokenizer.tokenization_time,\n",
    "        'training_time': training_time,\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': test_acc,\n",
    "    }\n",
    "    \n",
    "    return model, tokenizer, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ì˜ˆì¸¡ ë° í‰ê°€ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, device, max_len=100):\n",
    "    \"\"\"ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ í† í°í™”\n",
    "    tokens = tokenizer.tokenize([text])[0]\n",
    "    \n",
    "    # íŒ¨ë”© ì¶”ê°€\n",
    "    if len(tokens) < max_len:\n",
    "        tokens = tokens + [0] * (max_len - len(tokens))\n",
    "    else:\n",
    "        tokens = tokens[:max_len]\n",
    "    \n",
    "    # í…ì„œë¡œ ë³€í™˜\n",
    "    tokens_tensor = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "    \n",
    "    # ì˜ˆì¸¡\n",
    "    with torch.no_grad():\n",
    "        prediction = model(tokens_tensor)\n",
    "    \n",
    "    # ê²°ê³¼ ë°˜í™˜\n",
    "    prob = prediction.item()\n",
    "    sentiment = 1 if prob >= 0.5 else 0\n",
    "    \n",
    "    return sentiment, prob\n",
    "\n",
    "def compare_predictions(models_tokenizers, example_texts):\n",
    "    \"\"\"ì—¬ëŸ¬ ëª¨ë¸-í† í¬ë‚˜ì´ì € ìŒì— ëŒ€í•´ ì˜ˆì œ í…ìŠ¤íŠ¸ ì˜ˆì¸¡ ë¹„êµ\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    print(\"\\nì˜ˆì œ í…ìŠ¤íŠ¸ ì˜ˆì¸¡ ë¹„êµ:\")\n",
    "    for i, text in enumerate(example_texts, 1):\n",
    "        print(f\"\\n[ì˜ˆì œ {i}] {text}\")\n",
    "        \n",
    "        for model, tokenizer, tokenizer_name in models_tokenizers:\n",
    "            sentiment, prob = predict_sentiment(text, model, tokenizer, device)\n",
    "            print(f\"{tokenizer_name:15s}: {'ê¸ì •' if sentiment == 1 else 'ë¶€ì •'} (í™•ë¥ : {prob:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì‹œê°í™” í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(metrics_list):\n",
    "    \"\"\"í† í¬ë‚˜ì´ì €ë³„ ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”\"\"\"\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    plt.rcParams['font.family'] = 'AppleGothic, Arial'  # í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì €ë³„ ìƒ‰ìƒ ì„¤ì •\n",
    "    colors = {\n",
    "        'SentencePiece': '#3498DB',  # íŒŒë‘\n",
    "        'Okt': '#E74C3C',           # ë¹¨ê°•\n",
    "        'Kkma': '#2ECC71'           # ì´ˆë¡\n",
    "    }\n",
    "    \n",
    "    markers = {\n",
    "        'SentencePiece': 'o',  # ì›í˜•\n",
    "        'Okt': 's',           # ì‚¬ê°í˜•\n",
    "        'Kkma': '^'           # ì‚¼ê°í˜•\n",
    "    }\n",
    "    \n",
    "    # 1. ì •í™•ë„ ë¹„êµ\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for metrics in metrics_list:\n",
    "        name = metrics['name']\n",
    "        plt.plot(metrics['train_accuracies'], \n",
    "                 label=f\"{name} (í•™ìŠµ)\", \n",
    "                 color=colors[name], \n",
    "                 marker=markers[name],\n",
    "                 markersize=8)\n",
    "        plt.plot(metrics['val_accuracies'], \n",
    "                 label=f\"{name} (ê²€ì¦)\", \n",
    "                 color=colors[name], \n",
    "                 linestyle='--',\n",
    "                 marker=markers[name],\n",
    "                 markersize=6,\n",
    "                 alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('ì—í­', fontsize=12)\n",
    "    plt.ylabel('ì •í™•ë„', fontsize=12)\n",
    "    plt.title('í† í¬ë‚˜ì´ì €ë³„ í•™ìŠµ ë° ê²€ì¦ ì •í™•ë„ ë¹„êµ', fontsize=16, fontweight='bold')\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim([0.7, 1.0])  # ì •í™•ë„ ë²”ìœ„ ì¡°ì •\n",
    "    \n",
    "    # 2. ì†ì‹¤ ë¹„êµ\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for metrics in metrics_list:\n",
    "        name = metrics['name']\n",
    "        plt.plot(metrics['train_losses'], \n",
    "                 label=f\"{name} (í•™ìŠµ)\", \n",
    "                 color=colors[name], \n",
    "                 marker=markers[name],\n",
    "                 markersize=8)\n",
    "        plt.plot(metrics['val_losses'], \n",
    "                 label=f\"{name} (ê²€ì¦)\", \n",
    "                 color=colors[name], \n",
    "                 linestyle='--',\n",
    "                 marker=markers[name],\n",
    "                 markersize=6,\n",
    "                 alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('ì—í­', fontsize=12)\n",
    "    plt.ylabel('ì†ì‹¤', fontsize=12)\n",
    "    plt.title('í† í¬ë‚˜ì´ì €ë³„ í•™ìŠµ ë° ê²€ì¦ ì†ì‹¤ ë¹„êµ', fontsize=16, fontweight='bold')\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. í…ŒìŠ¤íŠ¸ ì •í™•ë„ ë¹„êµ (ë§‰ëŒ€ ê·¸ë˜í”„)\n",
    "    plt.subplot(2, 2, 3)\n",
    "    names = [m['name'] for m in metrics_list]\n",
    "    test_accs = [m['test_accuracy'] * 100 for m in metrics_list]\n",
    "    bar_colors = [colors[name] for name in names]\n",
    "    \n",
    "    bars = plt.bar(names, test_accs, color=bar_colors, width=0.6, edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "    plt.xlabel('í† í¬ë‚˜ì´ì €', fontsize=12)\n",
    "    plt.ylabel('ì •í™•ë„ (%)', fontsize=12)\n",
    "    plt.title('í…ŒìŠ¤íŠ¸ ì •í™•ë„ ë¹„êµ', fontsize=16, fontweight='bold')\n",
    "    plt.ylim([85, 100])  # Yì¶• ë²”ìœ„ ì¡°ì •\n",
    "    \n",
    "    # ì •í™•ë„ ìˆ˜ì¹˜ í‘œì‹œ\n",
    "    for bar, acc in zip(bars, test_accs):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "                f'{acc:.2f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 4. ì²˜ë¦¬ ì‹œê°„ ë¹„êµ (ë§‰ëŒ€ ê·¸ë˜í”„)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    tokenization_times = [m['tokenization_time'] for m in metrics_list]\n",
    "    training_times = [m['training_time'] for m in metrics_list]\n",
    "    \n",
    "    x = np.arange(len(names))\n",
    "    width = 0.35\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì € ì‹œê°„ì€ ë” íˆ¬ëª…í•˜ê²Œ, í•™ìŠµ ì‹œê°„ì€ ë” ì§„í•˜ê²Œ\n",
    "    plt.bar(x - width/2, tokenization_times, width, \n",
    "            label='í† í°í™” ì‹œê°„ (ì´ˆ)', alpha=0.7, \n",
    "            color=[colors[name] for name in names],\n",
    "            edgecolor='black', linewidth=1.5, hatch='/')\n",
    "            \n",
    "    plt.bar(x + width/2, training_times, width, \n",
    "            label='í•™ìŠµ ì‹œê°„ (ì´ˆ)', alpha=0.9, \n",
    "            color=[colors[name] for name in names],\n",
    "            edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    plt.xlabel('í† í¬ë‚˜ì´ì €', fontsize=12)\n",
    "    plt.ylabel('ì‹œê°„ (ì´ˆ)', fontsize=12)\n",
    "    plt.title('ì²˜ë¦¬ ì‹œê°„ ë¹„êµ', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(x, names, fontsize=12)\n",
    "    plt.legend(fontsize=10, loc='upper left')\n",
    "    \n",
    "    # ì‹œê°„ ìˆ˜ì¹˜ í‘œì‹œ (ì´ˆ ë‹¨ìœ„)\n",
    "    for i, time_val in enumerate(tokenization_times):\n",
    "        plt.text(i - width/2, time_val + 5, f'{time_val:.1f}ì´ˆ', ha='center', va='bottom', fontsize=10)\n",
    "    for i, time_val in enumerate(training_times):\n",
    "        plt.text(i + width/2, time_val + 5, f'{time_val:.1f}ì´ˆ', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # 5. ì¶”ê°€: ì–´íœ˜ í¬ê¸° ë¹„êµ (ë³„ë„ì˜ ê·¸ë˜í”„ë¡œ)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    vocab_sizes = [m['vocab_size'] for m in metrics_list]\n",
    "    bar_colors = [colors[name] for name in names]\n",
    "    \n",
    "    bars = plt.bar(names, vocab_sizes, color=bar_colors, width=0.6, edgecolor='black', linewidth=1.5)\n",
    "    plt.xlabel('í† í¬ë‚˜ì´ì €', fontsize=14)\n",
    "    plt.ylabel('ì–´íœ˜ í¬ê¸°', fontsize=14)\n",
    "    plt.title('í† í¬ë‚˜ì´ì €ë³„ ì–´íœ˜ í¬ê¸° ë¹„êµ', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # ë‹¨ì–´ ìˆ˜ í‘œì‹œ\n",
    "    for bar, size in zip(bars, vocab_sizes):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "                f'{size:,}', ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tokenizer_vocab_comparison.png')\n",
    "    \n",
    "    # ì›ë˜ ë¹„êµ ê·¸ë˜í”„ë¡œ ëŒì•„ê°€ê¸°\n",
    "    plt.figure(1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tokenizer_comparison.png')\n",
    "    plt.show()\n",
    "    \n",
    "# âœ… í† í°í™” ì˜ˆì‹œ ì‹œê°í™” \n",
    "def visualize_tokenization_examples(tokenizers):\n",
    "    \"\"\"ê° í† í¬ë‚˜ì´ì €ë³„ í† í°í™” ê²°ê³¼ ë¹„êµ ì‹œê°í™”\"\"\"\n",
    "    example_sentences = [\n",
    "        \"ì´ ì˜í™”ëŠ” ì •ë§ ì¬ë¯¸ìˆì—ˆì–´ìš”. ë°°ìš°ë“¤ì˜ ì—°ê¸°ë„ í›Œë¥­í–ˆìŠµë‹ˆë‹¤.\",\n",
    "        \"ìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ ë»”í•´ì„œ ì§€ë£¨í–ˆì–´ìš”.\"\n",
    "    ]\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.rcParams['font.family'] = 'AppleGothic, Arial'  # í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "    \n",
    "    colors = {\n",
    "        'SentencePiece': '#3498DB',  # íŒŒë‘\n",
    "        'Okt': '#E74C3C',           # ë¹¨ê°•\n",
    "        'Kkma': '#2ECC71'           # ì´ˆë¡\n",
    "    }\n",
    "    \n",
    "    # ê° ë¬¸ì¥ì— ëŒ€í•´\n",
    "    for idx, sentence in enumerate(example_sentences):\n",
    "        plt.subplot(len(example_sentences), 1, idx+1)\n",
    "        \n",
    "        y_positions = []\n",
    "        y_ticks = []\n",
    "        \n",
    "        # ê° í† í¬ë‚˜ì´ì €ë³„ë¡œ í† í°í™” ê²°ê³¼ ì‹œê°í™”\n",
    "        for i, (name, tokenizer) in enumerate(tokenizers.items()):\n",
    "            # í† í°í™”\n",
    "            if name == 'SentencePiece':\n",
    "                tokens = tokenizer.sp.EncodeAsPieces(sentence)\n",
    "            elif name == 'Okt':\n",
    "                tokens = tokenizer.okt.morphs(sentence, stem=True)\n",
    "            else:  # Kkma\n",
    "                tokens = tokenizer.kkma.morphs(sentence)\n",
    "            \n",
    "            # í† í°ë³„ ìœ„ì¹˜ ê³„ì‚°\n",
    "            x_positions = [0]\n",
    "            for token in tokens:\n",
    "                x_positions.append(x_positions[-1] + len(token) + 1)\n",
    "            x_positions = x_positions[:-1]  # ë§ˆì§€ë§‰ ìœ„ì¹˜ ì œê±°\n",
    "            \n",
    "            y_position = i * 2  # ê° í† í¬ë‚˜ì´ì €ëŠ” 2ì¹¸ì”© ê°„ê²©\n",
    "            y_positions.append(y_position)\n",
    "            y_ticks.append(y_position)\n",
    "            \n",
    "            # í† í° ì‹œê°í™”\n",
    "            for j, (token, x) in enumerate(zip(tokens, x_positions)):\n",
    "                plt.text(x, y_position, token, \n",
    "                         bbox=dict(facecolor=colors[name], alpha=0.3, edgecolor=colors[name], pad=5),\n",
    "                         fontsize=10)\n",
    "                \n",
    "                # í† í° ì‚¬ì´ì— í™”ì‚´í‘œ ì—°ê²°\n",
    "                if j < len(tokens) - 1:\n",
    "                    next_x = x_positions[j+1]\n",
    "                    plt.arrow(x + len(token) + 0.5, y_position, \n",
    "                              next_x - x - len(token) - 1, 0,\n",
    "                              head_width=0.1, head_length=0.3, fc=colors[name], ec=colors[name], alpha=0.6)\n",
    "        \n",
    "        # ì›ë³¸ ë¬¸ì¥ í‘œì‹œ\n",
    "        plt.text(0, max(y_positions) + 2, f\"ì›ë¬¸: {sentence}\", fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # ì¶• ì„¤ì •\n",
    "        plt.yticks(y_ticks, list(tokenizers.keys()))\n",
    "        plt.xticks([])\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "        plt.xlim(-1, max([len(sentence) * 1.5 for sentence in example_sentences]))\n",
    "        plt.ylim(-1, max(y_positions) + 3)\n",
    "        plt.title(f'ë¬¸ì¥ {idx+1} í† í°í™” ê²°ê³¼ ë¹„êµ', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tokenization_example.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ“Š SentencePiece, Okt, Kkma í† í¬ë‚˜ì´ì € ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜ ì‹œì‘\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "ì‹¤í—˜: SentencePiece í† í¬ë‚˜ì´ì € ì‚¬ìš©\n",
      "==================================================\n",
      "SentencePiece í† í°í™” ì‹œì‘...\n",
      "í† í°í™” ì†Œìš” ì‹œê°„: 0.58ì´ˆ\n",
      "SentencePiece ëª¨ë¸ í•™ìŠµ ì‹œì‘...\n",
      "Epoch 1: Best model saved with validation loss: 0.3466\n",
      "Epoch 1/5\n",
      "\tTrain Loss: 0.4286 | Train Acc: 79.74%\n",
      "\tVal Loss: 0.3466 | Val Acc: 84.49%\n",
      "Epoch 2: Best model saved with validation loss: 0.3314\n",
      "Epoch 2/5\n",
      "\tTrain Loss: 0.3017 | Train Acc: 87.11%\n",
      "\tVal Loss: 0.3314 | Val Acc: 85.58%\n",
      "Epoch 3: Best model saved with validation loss: 0.3271\n",
      "Epoch 3/5\n",
      "\tTrain Loss: 0.2406 | Train Acc: 90.03%\n",
      "\tVal Loss: 0.3271 | Val Acc: 85.92%\n",
      "Epoch 4/5\n",
      "\tTrain Loss: 0.1764 | Train Acc: 92.87%\n",
      "\tVal Loss: 0.3868 | Val Acc: 85.67%\n",
      "Epoch 5/5\n",
      "\tTrain Loss: 0.1239 | Train Acc: 95.16%\n",
      "\tVal Loss: 0.4298 | Val Acc: 85.52%\n",
      "í•™ìŠµ ì†Œìš” ì‹œê°„: 715.33ì´ˆ\n",
      "í…ŒìŠ¤íŠ¸ ê²°ê³¼: Loss: 0.3295 | Acc: 85.79%\n",
      "\n",
      "==================================================\n",
      "ì‹¤í—˜: Okt í† í¬ë‚˜ì´ì € ì‚¬ìš©\n",
      "==================================================\n",
      "Okt í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì‹œì‘...\n",
      "Okt ì–´íœ˜ ì‚¬ì „ êµ¬ì¶• ì™„ë£Œ: 27309 ë‹¨ì–´ (ì‹œê°„: 243.31ì´ˆ)\n",
      "Okt í† í°í™” ì‹œì‘...\n",
      "í† í°í™” ì†Œìš” ì‹œê°„: 89.61ì´ˆ\n",
      "Okt ëª¨ë¸ í•™ìŠµ ì‹œì‘...\n",
      "Epoch 1: Best model saved with validation loss: 0.3401\n",
      "Epoch 1/5\n",
      "\tTrain Loss: 0.4014 | Train Acc: 81.42%\n",
      "\tVal Loss: 0.3401 | Val Acc: 84.96%\n",
      "Epoch 2: Best model saved with validation loss: 0.3122\n",
      "Epoch 2/5\n",
      "\tTrain Loss: 0.2827 | Train Acc: 88.07%\n",
      "\tVal Loss: 0.3122 | Val Acc: 86.65%\n",
      "Epoch 3/5\n",
      "\tTrain Loss: 0.2127 | Train Acc: 91.43%\n",
      "\tVal Loss: 0.3322 | Val Acc: 86.54%\n",
      "Epoch 4/5\n",
      "\tTrain Loss: 0.1516 | Train Acc: 94.13%\n",
      "\tVal Loss: 0.3786 | Val Acc: 86.40%\n",
      "Epoch 5/5\n",
      "\tTrain Loss: 0.1064 | Train Acc: 95.95%\n",
      "\tVal Loss: 0.4079 | Val Acc: 86.24%\n",
      "í•™ìŠµ ì†Œìš” ì‹œê°„: 728.42ì´ˆ\n",
      "í…ŒìŠ¤íŠ¸ ê²°ê³¼: Loss: 0.3221 | Acc: 85.98%\n",
      "\n",
      "==================================================\n",
      "ì‹¤í—˜: Kkma í† í¬ë‚˜ì´ì € ì‚¬ìš©\n",
      "==================================================\n",
      "Kkma í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì‹œì‘...\n"
     ]
    },
    {
     "ename": "java.lang.OutOfMemoryError",
     "evalue": "java.lang.OutOfMemoryError: Java heap space",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32mkr.lucypark.kkma.KkmaInterface.java:0\u001b[0m, in \u001b[0;36mkr.lucypark.kkma.KkmaInterface.morphAnalyzer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Java Exception",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mjava.lang.OutOfMemoryError\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 90\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 38\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# 3. KoNLPy Kkma ì‹¤í—˜\u001b[39;00m\n\u001b[1;32m     37\u001b[0m kkma_tokenizer \u001b[38;5;241m=\u001b[39m KkmaTokenizer()\n\u001b[0;32m---> 38\u001b[0m kkma_model, kkma_tokenizer, kkma_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mKkma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkkma_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_data_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m     44\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 4. ê²°ê³¼ ë¹„êµ ë° ì‹œê°í™”\u001b[39;00m\n\u001b[1;32m     47\u001b[0m metrics_list \u001b[38;5;241m=\u001b[39m [sp_metrics, okt_metrics, kkma_metrics]\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(tokenizer_name, tokenizer, train_data, test_data, max_len, batch_size, epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì‹œì‘...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdocument\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 2. ë°ì´í„° í† í°í™”\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m í† í°í™” ì‹œì‘...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 141\u001b[0m, in \u001b[0;36mKkmaTokenizer.train\u001b[0;34m(self, texts, min_freq)\u001b[0m\n\u001b[1;32m    139\u001b[0m word_counts \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[0;32m--> 141\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkkma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmorphs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# í˜•íƒœì†Œ ë¶„ì„\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m word_counts:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/konlpy/tag/_kkma.py:95\u001b[0m, in \u001b[0;36mKkma.morphs\u001b[0;34m(self, phrase)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmorphs\u001b[39m(\u001b[38;5;28mself\u001b[39m, phrase):\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse phrase to morphemes.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [s \u001b[38;5;28;01mfor\u001b[39;00m s, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/konlpy/tag/_kkma.py:66\u001b[0m, in \u001b[0;36mKkma.pos\u001b[0;34m(self, phrase, flatten, join)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"POS tagger.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m:param flatten: If False, preserves eojeols.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m:param join: If True, returns joined sets of morph and tag.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m validate_phrase_inputs(phrase)\n\u001b[0;32m---> 66\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjki\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmorphAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m morphemes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentences:\n",
      "\u001b[0;31mjava.lang.OutOfMemoryError\u001b[0m: java.lang.OutOfMemoryError: Java heap space"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜: ì „ì²´ ì‹¤í—˜ ì‹¤í–‰\"\"\"\n",
    "    # ìƒ˜í”Œ ìˆ˜ ì œí•œ (í…ŒìŠ¤íŠ¸ìš©)\n",
    "    # ì „ì²´ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµí•  ë•ŒëŠ” ì£¼ì„ ì²˜ë¦¬\n",
    "    # train_data_sample = train_data.sample(n=10000).reset_index(drop=True)\n",
    "    # test_data_sample = test_data.sample(n=2000).reset_index(drop=True)\n",
    "    \n",
    "    # ì „ì²´ ë°ì´í„°ì…‹ ì‚¬ìš© (ì‹œê°„ ë‹¨ì¶•ì„ ìœ„í•´ ì¼ë¶€ë§Œ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŒ)\n",
    "    train_data_sample = train_data\n",
    "    test_data_sample = test_data\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ“Š SentencePiece, Okt, Kkma í† í¬ë‚˜ì´ì € ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜ ì‹œì‘\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. SentencePiece ì‹¤í—˜\n",
    "    sp_tokenizer = SentencePieceTokenizer(vocab_size=8000)\n",
    "    sp_model, sp_tokenizer, sp_metrics = run_experiment(\n",
    "        'SentencePiece', \n",
    "        sp_tokenizer, \n",
    "        train_data_sample, \n",
    "        test_data_sample,\n",
    "        epochs=5\n",
    "    )\n",
    "    \n",
    "    # 2. KoNLPy Okt ì‹¤í—˜\n",
    "    okt_tokenizer = OktTokenizer()\n",
    "    okt_model, okt_tokenizer, okt_metrics = run_experiment(\n",
    "        'Okt', \n",
    "        okt_tokenizer, \n",
    "        train_data_sample, \n",
    "        test_data_sample,\n",
    "        epochs=5\n",
    "    )\n",
    "    \n",
    "    # 3. KoNLPy Kkma ì‹¤í—˜\n",
    "    kkma_tokenizer = KkmaTokenizer()\n",
    "    kkma_model, kkma_tokenizer, kkma_metrics = run_experiment(\n",
    "        'Kkma', \n",
    "        kkma_tokenizer, \n",
    "        train_data_sample, \n",
    "        test_data_sample,\n",
    "        epochs=5\n",
    "    )\n",
    "    \n",
    "    # 4. ê²°ê³¼ ë¹„êµ ë° ì‹œê°í™”\n",
    "    metrics_list = [sp_metrics, okt_metrics, kkma_metrics]\n",
    "    plot_comparison(metrics_list)\n",
    "    \n",
    "    # 5. í† í°í™” ì˜ˆì‹œ ì‹œê°í™”\n",
    "    tokenizers = {\n",
    "        'SentencePiece': sp_tokenizer,\n",
    "        'Okt': okt_tokenizer,\n",
    "        'Kkma': kkma_tokenizer\n",
    "    }\n",
    "    visualize_tokenization_examples(tokenizers)\n",
    "    \n",
    "    # 6. ì˜ˆì œ í…ìŠ¤íŠ¸ ì˜ˆì¸¡ ë¹„êµ\n",
    "    example_texts = [\n",
    "        \"ì´ ì˜í™”ëŠ” ì •ë§ ì¬ë¯¸ìˆì—ˆì–´ìš”. ë°°ìš°ë“¤ì˜ ì—°ê¸°ë„ í›Œë¥­í–ˆê³  ìŠ¤í† ë¦¬ë„ ì¢‹ì•˜ìŠµë‹ˆë‹¤.\",\n",
    "        \"ì‹œê°„ ë‚­ë¹„ì˜€ìŠµë‹ˆë‹¤. ìŠ¤í† ë¦¬ë„ ì—‰ë§ì´ê³  ë°°ìš°ë“¤ì˜ ì—°ê¸°ë„ ë³„ë¡œì˜€ì–´ìš”.\",\n",
    "        \"ê·¸ëŸ­ì €ëŸ­ ë³¼ë§Œí–ˆì–´ìš”. íŠ¹ë³„íˆ ì¢‹ì§€ë„ ë‚˜ì˜ì§€ë„ ì•Šì•˜ìŠµë‹ˆë‹¤.\",\n",
    "        \"ì—°ì¶œì€ ì¢‹ì•˜ì§€ë§Œ ìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ ë»”í•´ì„œ ëª°ì…ì´ ì•ˆëì–´ìš”.\",\n",
    "        \"ë°°ìš°ë“¤ì˜ ì—°ê¸°ëŠ” í›Œë¥­í–ˆì§€ë§Œ ë„ˆë¬´ ì§€ë£¨í–ˆì–´ìš”.\"\n",
    "    ]\n",
    "    \n",
    "    models_tokenizers = [\n",
    "        (sp_model, sp_tokenizer, 'SentencePiece'),\n",
    "        (okt_model, okt_tokenizer, 'Okt'),\n",
    "        (kkma_model, kkma_tokenizer, 'Kkma')\n",
    "    ]\n",
    "    \n",
    "    compare_predictions(models_tokenizers, example_texts)\n",
    "    \n",
    "    # 7. ì„±ëŠ¥ ìš”ì•½ ì¶œë ¥\n",
    "    print(\"\\nì„±ëŠ¥ ìš”ì•½:\")\n",
    "    print(\"-\" * 50)\n",
    "    for metrics in metrics_list:\n",
    "        print(f\"ğŸ“Œ {metrics['name']} í† í¬ë‚˜ì´ì €:\")\n",
    "        print(f\"  â€¢ ì–´íœ˜ í¬ê¸°: {metrics['vocab_size']:,} ë‹¨ì–´\")\n",
    "        print(f\"  â€¢ í† í°í™” ì‹œê°„: {metrics['tokenization_time']:.2f}ì´ˆ\")\n",
    "        print(f\"  â€¢ í•™ìŠµ ì‹œê°„: {metrics['training_time']:.2f}ì´ˆ\")\n",
    "        print(f\"  â€¢ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {metrics['test_accuracy']*100:.2f}%\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(\"\\nì‹¤í—˜ì´ ëª¨ë‘ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê²°ê³¼ëŠ” PNG íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
