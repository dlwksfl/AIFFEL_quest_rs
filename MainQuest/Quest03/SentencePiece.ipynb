{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 영화리뷰 감정 분석 문제에 SentencePiece 적용해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: (149995, 3)\n",
      "Test Data: (49997, 3)\n"
     ]
    }
   ],
   "source": [
    "# 네이버 영화 리뷰 데이터셋을 로드하고 전처리하는 과정\n",
    "train_txt = '/Users/jian_lee/Desktop/aiffel/data/text_process/Data/ratings_train.txt'\n",
    "test_txt = '/Users/jian_lee/Desktop/aiffel/data/text_process/Data/ratings_test.txt'\n",
    "\n",
    "def load_data(file):\n",
    "    \"\"\"\n",
    "    텍스트 파일을 로드하여 DataFrame으로 변환하는 함수\n",
    "    - 첫 번째 컬럼은 id, 두 번째 컬럼은 리뷰 내용(document), 세 번째 컬럼은 라벨(label)이다.\n",
    "    - 첫 줄은 헤더이므로 제거하고, id 컬럼은 사용하지 않는다.\n",
    "    - 결측치를 제거하고 데이터를 랜덤하게 섞는다.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file, delimiter='\\t', names=['id', 'document', 'label'], skiprows=1)\n",
    "    df.dropna(inplace=True)  # 결측치 제거\n",
    "    df = df.sample(frac=1).reset_index(drop=True)  # 데이터 랜덤 섞기\n",
    "    return df\n",
    "\n",
    "# 훈련 및 테스트 데이터 로드\n",
    "train_data = load_data(train_txt)\n",
    "test_data = load_data(test_txt)\n",
    "\n",
    "print(f\"Train Data: {train_data.shape}\")\n",
    "print(f\"Test Data: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 토크나이저 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 SentencePiece 토크나이저 클래스\n",
    "class SentencePieceTokenizer:\n",
    "    def __init__(self, vocab_size=8000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sp_model_prefix = 'spm_review'\n",
    "        self.sp_model_path = f'{self.sp_model_prefix}.model'\n",
    "        self.sp = None\n",
    "        self.vocab_size_actual = None\n",
    "        self.tokenization_time = 0\n",
    "    \n",
    "    def train(self, texts):\n",
    "        \"\"\"SentencePiece 모델 학습\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 훈련 데이터 준비\n",
    "        with open('spm_input.txt', 'w', encoding='utf-8') as f:\n",
    "            for text in texts:\n",
    "                f.write(text + '\\n')\n",
    "        \n",
    "        # 모델 훈련\n",
    "        spm.SentencePieceTrainer.Train(\n",
    "            input='spm_input.txt',\n",
    "            model_prefix=self.sp_model_prefix,\n",
    "            vocab_size=self.vocab_size,\n",
    "            model_type='bpe',\n",
    "            character_coverage=0.9995,\n",
    "        )\n",
    "        \n",
    "        # 모델 로드\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.Load(self.sp_model_path)\n",
    "        self.vocab_size_actual = self.sp.GetPieceSize()\n",
    "        \n",
    "        self.tokenization_time = time.time() - start_time\n",
    "        print(f\"SentencePiece 모델 학습 완료 (시간: {self.tokenization_time:.2f}초)\")\n",
    "    \n",
    "    def load(self):\n",
    "        \"\"\"학습된 SentencePiece 모델 로드\"\"\"\n",
    "        if os.path.exists(self.sp_model_path):\n",
    "            self.sp = spm.SentencePieceProcessor()\n",
    "            self.sp.Load(self.sp_model_path)\n",
    "            self.vocab_size_actual = self.sp.GetPieceSize()\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def tokenize(self, texts):\n",
    "        \"\"\"텍스트 리스트를 토큰화\"\"\"\n",
    "        start_time = time.time()\n",
    "        tokenized = [self.sp.EncodeAsIds(text) for text in texts]\n",
    "        self.tokenization_time = time.time() - start_time\n",
    "        return tokenized\n",
    "    \n",
    "    def detokenize(self, token_ids):\n",
    "        \"\"\"토큰 ID 리스트를 텍스트로 변환\"\"\"\n",
    "        if isinstance(token_ids[0], list):\n",
    "            return [self.sp.DecodeIds(ids) for ids in token_ids]\n",
    "        return self.sp.DecodeIds(token_ids)\n",
    "\n",
    "# 2.2 KoNLPy Okt 토크나이저 클래스\n",
    "class OktTokenizer:\n",
    "    def __init__(self):\n",
    "        self.okt = Okt()\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.vocab_size = 0\n",
    "        self.tokenization_time = 0\n",
    "    \n",
    "    def train(self, texts, min_freq=2):\n",
    "        \"\"\"어휘 사전 구축\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Okt 토크나이저 초기화\n",
    "        self.okt = Okt()\n",
    "        \n",
    "        # 단어 빈도수 계산\n",
    "        word_counts = {}\n",
    "        for text in texts:\n",
    "            tokens = self.okt.morphs(text, stem=True)  # 형태소 분석 (스테밍 적용)\n",
    "            for token in tokens:\n",
    "                if token not in word_counts:\n",
    "                    word_counts[token] = 0\n",
    "                word_counts[token] += 1\n",
    "        \n",
    "        # 최소 빈도수 이상 단어만 사전에 추가\n",
    "        vocab = ['<PAD>', '<UNK>']  # 패딩과 알 수 없는 단어를 위한 특수 토큰\n",
    "        vocab.extend([word for word, count in word_counts.items() if count >= min_freq])\n",
    "        \n",
    "        # 단어-인덱스 매핑 구축\n",
    "        self.word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "        self.index_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "        self.vocab_size = len(vocab)\n",
    "        \n",
    "        self.tokenization_time = time.time() - start_time\n",
    "        print(f\"Okt 어휘 사전 구축 완료: {self.vocab_size} 단어 (시간: {self.tokenization_time:.2f}초)\")\n",
    "    \n",
    "    def tokenize(self, texts):\n",
    "        \"\"\"텍스트 리스트를 토큰화하여 인덱스 리스트로 변환\"\"\"\n",
    "        start_time = time.time()\n",
    "        tokenized = []\n",
    "        for text in texts:\n",
    "            tokens = self.okt.morphs(text, stem=True)\n",
    "            indices = [self.word_to_index.get(token, 1) for token in tokens]  # 1은 <UNK> 토큰\n",
    "            tokenized.append(indices)\n",
    "        \n",
    "        self.tokenization_time = time.time() - start_time\n",
    "        return tokenized\n",
    "    \n",
    "    def detokenize(self, token_ids):\n",
    "        \"\"\"토큰 ID 리스트를 텍스트로 변환\"\"\"\n",
    "        if isinstance(token_ids[0], list):\n",
    "            texts = []\n",
    "            for ids in token_ids:\n",
    "                tokens = [self.index_to_word.get(idx, '<UNK>') for idx in ids if idx != 0]  # 0은 <PAD> 토큰\n",
    "                texts.append(' '.join(tokens))\n",
    "            return texts\n",
    "        else:\n",
    "            tokens = [self.index_to_word.get(idx, '<UNK>') for idx in token_ids if idx != 0]\n",
    "            return ' '.join(tokens)\n",
    "\n",
    "# 2.3 KoNLPy Kkma 토크나이저 클래스\n",
    "class KkmaTokenizer:\n",
    "    def __init__(self):\n",
    "        from konlpy.tag import Kkma\n",
    "        self.kkma = Kkma()\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.vocab_size = 0\n",
    "        self.tokenization_time = 0\n",
    "    \n",
    "    def train(self, texts, min_freq=2):\n",
    "        \"\"\"어휘 사전 구축\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Kkma 토크나이저 초기화\n",
    "        from konlpy.tag import Kkma\n",
    "        self.kkma = Kkma()\n",
    "        \n",
    "        # 단어 빈도수 계산\n",
    "        word_counts = {}\n",
    "        for text in texts:\n",
    "            tokens = self.kkma.morphs(text)  # 형태소 분석\n",
    "            for token in tokens:\n",
    "                if token not in word_counts:\n",
    "                    word_counts[token] = 0\n",
    "                word_counts[token] += 1\n",
    "        \n",
    "        # 최소 빈도수 이상 단어만 사전에 추가\n",
    "        vocab = ['<PAD>', '<UNK>']  # 패딩과 알 수 없는 단어를 위한 특수 토큰\n",
    "        vocab.extend([word for word, count in word_counts.items() if count >= min_freq])\n",
    "        \n",
    "        # 단어-인덱스 매핑 구축\n",
    "        self.word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "        self.index_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "        self.vocab_size = len(vocab)\n",
    "        \n",
    "        self.tokenization_time = time.time() - start_time\n",
    "        print(f\"Kkma 어휘 사전 구축 완료: {self.vocab_size} 단어 (시간: {self.tokenization_time:.2f}초)\")\n",
    "    \n",
    "    def tokenize(self, texts):\n",
    "        \"\"\"텍스트 리스트를 토큰화하여 인덱스 리스트로 변환\"\"\"\n",
    "        start_time = time.time()\n",
    "        tokenized = []\n",
    "        for text in texts:\n",
    "            tokens = self.kkma.morphs(text)\n",
    "            indices = [self.word_to_index.get(token, 1) for token in tokens]  # 1은 <UNK> 토큰\n",
    "            tokenized.append(indices)\n",
    "        \n",
    "        self.tokenization_time = time.time() - start_time\n",
    "        return tokenized\n",
    "    \n",
    "    def detokenize(self, token_ids):\n",
    "        \"\"\"토큰 ID 리스트를 텍스트로 변환\"\"\"\n",
    "        if isinstance(token_ids[0], list):\n",
    "            texts = []\n",
    "            for ids in token_ids:\n",
    "                tokens = [self.index_to_word.get(idx, '<UNK>') for idx in ids if idx != 0]  # 0은 <PAD> 토큰\n",
    "                texts.append(' '.join(tokens))\n",
    "            return texts\n",
    "        else:\n",
    "            tokens = [self.index_to_word.get(idx, '<UNK>') for idx in token_ids if idx != 0]\n",
    "            return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 감정 분석 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout_rate=0.5):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=n_layers, \n",
    "            bidirectional=True, \n",
    "            dropout=dropout_rate if n_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        packed_output, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        hidden = self.dropout(hidden)\n",
    "        output = self.fc(hidden)\n",
    "        return self.sigmoid(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 학습 및 평가 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
    "    \"\"\"한 에폭 동안 모델을 학습시키는 함수\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for texts, labels in data_loader:\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.float().unsqueeze(1).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(texts)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        predicted_labels = (predictions >= 0.5).int()\n",
    "        correct_predictions += (predicted_labels == labels.int()).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    \"\"\"모델을 평가하는 함수\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_loader:\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            \n",
    "            predictions = model(texts)\n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predicted_labels = (predictions >= 0.5).int()\n",
    "            correct_predictions += (predicted_labels == labels.int()).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 실험 실행 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(tokenizer_name, tokenizer, train_data, test_data, max_len=100, batch_size=64, epochs=5):\n",
    "    \"\"\"특정 토크나이저를 사용한 실험을 수행하는 함수\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"실험: {tokenizer_name} 토크나이저 사용\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # 1. 토크나이저 훈련 또는 로드\n",
    "    if tokenizer_name == 'SentencePiece':\n",
    "        if not tokenizer.load():\n",
    "            print(\"SentencePiece 모델 훈련 시작...\")\n",
    "            tokenizer.train(train_data['document'])\n",
    "    else:\n",
    "        print(f\"{tokenizer_name} 토크나이저 준비 시작...\")\n",
    "        tokenizer.train(train_data['document'])\n",
    "    \n",
    "    # 2. 데이터 토큰화\n",
    "    print(f\"{tokenizer_name} 토큰화 시작...\")\n",
    "    train_texts = tokenizer.tokenize(train_data['document'])\n",
    "    test_texts = tokenizer.tokenize(test_data['document'])\n",
    "    \n",
    "    # 토큰화 소요 시간 기록\n",
    "    print(f\"토큰화 소요 시간: {tokenizer.tokenization_time:.2f}초\")\n",
    "    \n",
    "    # 3. 패딩 처리\n",
    "    train_texts = pad_sequences(train_texts, maxlen=max_len, padding='post')\n",
    "    test_texts = pad_sequences(test_texts, maxlen=max_len, padding='post')\n",
    "    \n",
    "    # 4. PyTorch 데이터셋 및 데이터로더 생성\n",
    "    train_texts_tensor = torch.tensor(train_texts, dtype=torch.long)\n",
    "    train_labels_tensor = torch.tensor(train_data['label'].values, dtype=torch.long)\n",
    "    test_texts_tensor = torch.tensor(test_texts, dtype=torch.long)\n",
    "    test_labels_tensor = torch.tensor(test_data['label'].values, dtype=torch.long)\n",
    "    \n",
    "    # 검증 데이터 분리\n",
    "    train_size = int(0.8 * len(train_texts_tensor))\n",
    "    indices = torch.randperm(len(train_texts_tensor))\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "    \n",
    "    train_dataset = TensorDataset(\n",
    "        train_texts_tensor[train_indices], \n",
    "        train_labels_tensor[train_indices]\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        train_texts_tensor[val_indices], \n",
    "        train_labels_tensor[val_indices]\n",
    "    )\n",
    "    test_dataset = TensorDataset(test_texts_tensor, test_labels_tensor)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # 5. 모델 설정\n",
    "    vocab_size = tokenizer.vocab_size_actual if hasattr(tokenizer, 'vocab_size_actual') else tokenizer.vocab_size\n",
    "    vocab_size = vocab_size + 1 if tokenizer_name == 'SentencePiece' else vocab_size  # SentencePiece는 <unk> 토큰 포함\n",
    "    \n",
    "    embedding_dim = 300\n",
    "    hidden_dim = 256\n",
    "    n_layers = 2\n",
    "    dropout_rate = 0.5\n",
    "    \n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() \n",
    "                      else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    model = SentimentLSTM(vocab_size, embedding_dim, hidden_dim, n_layers, dropout_rate).to(device)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # 6. 학습 실행\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = f'best_sentiment_model_{tokenizer_name}.pt'\n",
    "    \n",
    "    print(f\"{tokenizer_name} 모델 학습 시작...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        \n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Epoch {epoch+1}: Best model saved with validation loss: {val_loss:.4f}\")\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "        print(f\"\\tVal Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"학습 소요 시간: {training_time:.2f}초\")\n",
    "    \n",
    "    # 7. 최적 모델 로드 및 테스트\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    print(f\"테스트 결과: Loss: {test_loss:.4f} | Acc: {test_acc*100:.2f}%\")\n",
    "    \n",
    "    # 8. 시각화 데이터 및 성능 메트릭 반환\n",
    "    metrics = {\n",
    "        'name': tokenizer_name,\n",
    "        'vocab_size': vocab_size,\n",
    "        'tokenization_time': tokenizer.tokenization_time,\n",
    "        'training_time': training_time,\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': test_acc,\n",
    "    }\n",
    "    \n",
    "    return model, tokenizer, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 예측 및 평가 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, device, max_len=100):\n",
    "    \"\"\"새로운 텍스트의 감정을 예측하는 함수\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 텍스트 토큰화\n",
    "    tokens = tokenizer.tokenize([text])[0]\n",
    "    \n",
    "    # 패딩 추가\n",
    "    if len(tokens) < max_len:\n",
    "        tokens = tokens + [0] * (max_len - len(tokens))\n",
    "    else:\n",
    "        tokens = tokens[:max_len]\n",
    "    \n",
    "    # 텐서로 변환\n",
    "    tokens_tensor = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 예측\n",
    "    with torch.no_grad():\n",
    "        prediction = model(tokens_tensor)\n",
    "    \n",
    "    # 결과 반환\n",
    "    prob = prediction.item()\n",
    "    sentiment = 1 if prob >= 0.5 else 0\n",
    "    \n",
    "    return sentiment, prob\n",
    "\n",
    "def compare_predictions(models_tokenizers, example_texts):\n",
    "    \"\"\"여러 모델-토크나이저 쌍에 대해 예제 텍스트 예측 비교\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    print(\"\\n예제 텍스트 예측 비교:\")\n",
    "    for i, text in enumerate(example_texts, 1):\n",
    "        print(f\"\\n[예제 {i}] {text}\")\n",
    "        \n",
    "        for model, tokenizer, tokenizer_name in models_tokenizers:\n",
    "            sentiment, prob = predict_sentiment(text, model, tokenizer, device)\n",
    "            print(f\"{tokenizer_name:15s}: {'긍정' if sentiment == 1 else '부정'} (확률: {prob:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 시각화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(metrics_list):\n",
    "    \"\"\"토크나이저별 성능 비교 시각화\"\"\"\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    plt.rcParams['font.family'] = 'AppleGothic, Arial'  # 한글 폰트 설정\n",
    "    \n",
    "    # 토크나이저별 색상 설정\n",
    "    colors = {\n",
    "        'SentencePiece': '#3498DB',  # 파랑\n",
    "        'Okt': '#E74C3C',           # 빨강\n",
    "        'Kkma': '#2ECC71'           # 초록\n",
    "    }\n",
    "    \n",
    "    markers = {\n",
    "        'SentencePiece': 'o',  # 원형\n",
    "        'Okt': 's',           # 사각형\n",
    "        'Kkma': '^'           # 삼각형\n",
    "    }\n",
    "    \n",
    "    # 1. 정확도 비교\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for metrics in metrics_list:\n",
    "        name = metrics['name']\n",
    "        plt.plot(metrics['train_accuracies'], \n",
    "                 label=f\"{name} (학습)\", \n",
    "                 color=colors[name], \n",
    "                 marker=markers[name],\n",
    "                 markersize=8)\n",
    "        plt.plot(metrics['val_accuracies'], \n",
    "                 label=f\"{name} (검증)\", \n",
    "                 color=colors[name], \n",
    "                 linestyle='--',\n",
    "                 marker=markers[name],\n",
    "                 markersize=6,\n",
    "                 alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('에폭', fontsize=12)\n",
    "    plt.ylabel('정확도', fontsize=12)\n",
    "    plt.title('토크나이저별 학습 및 검증 정확도 비교', fontsize=16, fontweight='bold')\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim([0.7, 1.0])  # 정확도 범위 조정\n",
    "    \n",
    "    # 2. 손실 비교\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for metrics in metrics_list:\n",
    "        name = metrics['name']\n",
    "        plt.plot(metrics['train_losses'], \n",
    "                 label=f\"{name} (학습)\", \n",
    "                 color=colors[name], \n",
    "                 marker=markers[name],\n",
    "                 markersize=8)\n",
    "        plt.plot(metrics['val_losses'], \n",
    "                 label=f\"{name} (검증)\", \n",
    "                 color=colors[name], \n",
    "                 linestyle='--',\n",
    "                 marker=markers[name],\n",
    "                 markersize=6,\n",
    "                 alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('에폭', fontsize=12)\n",
    "    plt.ylabel('손실', fontsize=12)\n",
    "    plt.title('토크나이저별 학습 및 검증 손실 비교', fontsize=16, fontweight='bold')\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. 테스트 정확도 비교 (막대 그래프)\n",
    "    plt.subplot(2, 2, 3)\n",
    "    names = [m['name'] for m in metrics_list]\n",
    "    test_accs = [m['test_accuracy'] * 100 for m in metrics_list]\n",
    "    bar_colors = [colors[name] for name in names]\n",
    "    \n",
    "    bars = plt.bar(names, test_accs, color=bar_colors, width=0.6, edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "    plt.xlabel('토크나이저', fontsize=12)\n",
    "    plt.ylabel('정확도 (%)', fontsize=12)\n",
    "    plt.title('테스트 정확도 비교', fontsize=16, fontweight='bold')\n",
    "    plt.ylim([85, 100])  # Y축 범위 조정\n",
    "    \n",
    "    # 정확도 수치 표시\n",
    "    for bar, acc in zip(bars, test_accs):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "                f'{acc:.2f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 4. 처리 시간 비교 (막대 그래프)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    tokenization_times = [m['tokenization_time'] for m in metrics_list]\n",
    "    training_times = [m['training_time'] for m in metrics_list]\n",
    "    \n",
    "    x = np.arange(len(names))\n",
    "    width = 0.35\n",
    "    \n",
    "    # 토크나이저 시간은 더 투명하게, 학습 시간은 더 진하게\n",
    "    plt.bar(x - width/2, tokenization_times, width, \n",
    "            label='토큰화 시간 (초)', alpha=0.7, \n",
    "            color=[colors[name] for name in names],\n",
    "            edgecolor='black', linewidth=1.5, hatch='/')\n",
    "            \n",
    "    plt.bar(x + width/2, training_times, width, \n",
    "            label='학습 시간 (초)', alpha=0.9, \n",
    "            color=[colors[name] for name in names],\n",
    "            edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    plt.xlabel('토크나이저', fontsize=12)\n",
    "    plt.ylabel('시간 (초)', fontsize=12)\n",
    "    plt.title('처리 시간 비교', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(x, names, fontsize=12)\n",
    "    plt.legend(fontsize=10, loc='upper left')\n",
    "    \n",
    "    # 시간 수치 표시 (초 단위)\n",
    "    for i, time_val in enumerate(tokenization_times):\n",
    "        plt.text(i - width/2, time_val + 5, f'{time_val:.1f}초', ha='center', va='bottom', fontsize=10)\n",
    "    for i, time_val in enumerate(training_times):\n",
    "        plt.text(i + width/2, time_val + 5, f'{time_val:.1f}초', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # 5. 추가: 어휘 크기 비교 (별도의 그래프로)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    vocab_sizes = [m['vocab_size'] for m in metrics_list]\n",
    "    bar_colors = [colors[name] for name in names]\n",
    "    \n",
    "    bars = plt.bar(names, vocab_sizes, color=bar_colors, width=0.6, edgecolor='black', linewidth=1.5)\n",
    "    plt.xlabel('토크나이저', fontsize=14)\n",
    "    plt.ylabel('어휘 크기', fontsize=14)\n",
    "    plt.title('토크나이저별 어휘 크기 비교', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # 단어 수 표시\n",
    "    for bar, size in zip(bars, vocab_sizes):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "                f'{size:,}', ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tokenizer_vocab_comparison.png')\n",
    "    \n",
    "    # 원래 비교 그래프로 돌아가기\n",
    "    plt.figure(1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tokenizer_comparison.png')\n",
    "    plt.show()\n",
    "    \n",
    "# ✅ 토큰화 예시 시각화 \n",
    "def visualize_tokenization_examples(tokenizers):\n",
    "    \"\"\"각 토크나이저별 토큰화 결과 비교 시각화\"\"\"\n",
    "    example_sentences = [\n",
    "        \"이 영화는 정말 재미있었어요. 배우들의 연기도 훌륭했습니다.\",\n",
    "        \"스토리가 너무 뻔해서 지루했어요.\"\n",
    "    ]\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.rcParams['font.family'] = 'AppleGothic, Arial'  # 한글 폰트 설정\n",
    "    \n",
    "    colors = {\n",
    "        'SentencePiece': '#3498DB',  # 파랑\n",
    "        'Okt': '#E74C3C',           # 빨강\n",
    "        'Kkma': '#2ECC71'           # 초록\n",
    "    }\n",
    "    \n",
    "    # 각 문장에 대해\n",
    "    for idx, sentence in enumerate(example_sentences):\n",
    "        plt.subplot(len(example_sentences), 1, idx+1)\n",
    "        \n",
    "        y_positions = []\n",
    "        y_ticks = []\n",
    "        \n",
    "        # 각 토크나이저별로 토큰화 결과 시각화\n",
    "        for i, (name, tokenizer) in enumerate(tokenizers.items()):\n",
    "            # 토큰화\n",
    "            if name == 'SentencePiece':\n",
    "                tokens = tokenizer.sp.EncodeAsPieces(sentence)\n",
    "            elif name == 'Okt':\n",
    "                tokens = tokenizer.okt.morphs(sentence, stem=True)\n",
    "            else:  # Kkma\n",
    "                tokens = tokenizer.kkma.morphs(sentence)\n",
    "            \n",
    "            # 토큰별 위치 계산\n",
    "            x_positions = [0]\n",
    "            for token in tokens:\n",
    "                x_positions.append(x_positions[-1] + len(token) + 1)\n",
    "            x_positions = x_positions[:-1]  # 마지막 위치 제거\n",
    "            \n",
    "            y_position = i * 2  # 각 토크나이저는 2칸씩 간격\n",
    "            y_positions.append(y_position)\n",
    "            y_ticks.append(y_position)\n",
    "            \n",
    "            # 토큰 시각화\n",
    "            for j, (token, x) in enumerate(zip(tokens, x_positions)):\n",
    "                plt.text(x, y_position, token, \n",
    "                         bbox=dict(facecolor=colors[name], alpha=0.3, edgecolor=colors[name], pad=5),\n",
    "                         fontsize=10)\n",
    "                \n",
    "                # 토큰 사이에 화살표 연결\n",
    "                if j < len(tokens) - 1:\n",
    "                    next_x = x_positions[j+1]\n",
    "                    plt.arrow(x + len(token) + 0.5, y_position, \n",
    "                              next_x - x - len(token) - 1, 0,\n",
    "                              head_width=0.1, head_length=0.3, fc=colors[name], ec=colors[name], alpha=0.6)\n",
    "        \n",
    "        # 원본 문장 표시\n",
    "        plt.text(0, max(y_positions) + 2, f\"원문: {sentence}\", fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # 축 설정\n",
    "        plt.yticks(y_ticks, list(tokenizers.keys()))\n",
    "        plt.xticks([])\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "        plt.xlim(-1, max([len(sentence) * 1.5 for sentence in example_sentences]))\n",
    "        plt.ylim(-1, max(y_positions) + 3)\n",
    "        plt.title(f'문장 {idx+1} 토큰화 결과 비교', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tokenization_example.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📊 SentencePiece, Okt, Kkma 토크나이저 성능 비교 실험 시작\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "실험: SentencePiece 토크나이저 사용\n",
      "==================================================\n",
      "SentencePiece 토큰화 시작...\n",
      "토큰화 소요 시간: 0.58초\n",
      "SentencePiece 모델 학습 시작...\n",
      "Epoch 1: Best model saved with validation loss: 0.3466\n",
      "Epoch 1/5\n",
      "\tTrain Loss: 0.4286 | Train Acc: 79.74%\n",
      "\tVal Loss: 0.3466 | Val Acc: 84.49%\n",
      "Epoch 2: Best model saved with validation loss: 0.3314\n",
      "Epoch 2/5\n",
      "\tTrain Loss: 0.3017 | Train Acc: 87.11%\n",
      "\tVal Loss: 0.3314 | Val Acc: 85.58%\n",
      "Epoch 3: Best model saved with validation loss: 0.3271\n",
      "Epoch 3/5\n",
      "\tTrain Loss: 0.2406 | Train Acc: 90.03%\n",
      "\tVal Loss: 0.3271 | Val Acc: 85.92%\n",
      "Epoch 4/5\n",
      "\tTrain Loss: 0.1764 | Train Acc: 92.87%\n",
      "\tVal Loss: 0.3868 | Val Acc: 85.67%\n",
      "Epoch 5/5\n",
      "\tTrain Loss: 0.1239 | Train Acc: 95.16%\n",
      "\tVal Loss: 0.4298 | Val Acc: 85.52%\n",
      "학습 소요 시간: 715.33초\n",
      "테스트 결과: Loss: 0.3295 | Acc: 85.79%\n",
      "\n",
      "==================================================\n",
      "실험: Okt 토크나이저 사용\n",
      "==================================================\n",
      "Okt 토크나이저 준비 시작...\n",
      "Okt 어휘 사전 구축 완료: 27309 단어 (시간: 243.31초)\n",
      "Okt 토큰화 시작...\n",
      "토큰화 소요 시간: 89.61초\n",
      "Okt 모델 학습 시작...\n",
      "Epoch 1: Best model saved with validation loss: 0.3401\n",
      "Epoch 1/5\n",
      "\tTrain Loss: 0.4014 | Train Acc: 81.42%\n",
      "\tVal Loss: 0.3401 | Val Acc: 84.96%\n",
      "Epoch 2: Best model saved with validation loss: 0.3122\n",
      "Epoch 2/5\n",
      "\tTrain Loss: 0.2827 | Train Acc: 88.07%\n",
      "\tVal Loss: 0.3122 | Val Acc: 86.65%\n",
      "Epoch 3/5\n",
      "\tTrain Loss: 0.2127 | Train Acc: 91.43%\n",
      "\tVal Loss: 0.3322 | Val Acc: 86.54%\n",
      "Epoch 4/5\n",
      "\tTrain Loss: 0.1516 | Train Acc: 94.13%\n",
      "\tVal Loss: 0.3786 | Val Acc: 86.40%\n",
      "Epoch 5/5\n",
      "\tTrain Loss: 0.1064 | Train Acc: 95.95%\n",
      "\tVal Loss: 0.4079 | Val Acc: 86.24%\n",
      "학습 소요 시간: 728.42초\n",
      "테스트 결과: Loss: 0.3221 | Acc: 85.98%\n",
      "\n",
      "==================================================\n",
      "실험: Kkma 토크나이저 사용\n",
      "==================================================\n",
      "Kkma 토크나이저 준비 시작...\n"
     ]
    },
    {
     "ename": "java.lang.OutOfMemoryError",
     "evalue": "java.lang.OutOfMemoryError: Java heap space",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32mkr.lucypark.kkma.KkmaInterface.java:0\u001b[0m, in \u001b[0;36mkr.lucypark.kkma.KkmaInterface.morphAnalyzer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Java Exception",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mjava.lang.OutOfMemoryError\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 90\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# 메인 함수 실행\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 38\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# 3. KoNLPy Kkma 실험\u001b[39;00m\n\u001b[1;32m     37\u001b[0m kkma_tokenizer \u001b[38;5;241m=\u001b[39m KkmaTokenizer()\n\u001b[0;32m---> 38\u001b[0m kkma_model, kkma_tokenizer, kkma_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mKkma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkkma_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_data_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m     44\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 4. 결과 비교 및 시각화\u001b[39;00m\n\u001b[1;32m     47\u001b[0m metrics_list \u001b[38;5;241m=\u001b[39m [sp_metrics, okt_metrics, kkma_metrics]\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(tokenizer_name, tokenizer, train_data, test_data, max_len, batch_size, epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 토크나이저 준비 시작...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdocument\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 2. 데이터 토큰화\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 토큰화 시작...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 141\u001b[0m, in \u001b[0;36mKkmaTokenizer.train\u001b[0;34m(self, texts, min_freq)\u001b[0m\n\u001b[1;32m    139\u001b[0m word_counts \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[0;32m--> 141\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkkma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmorphs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 형태소 분석\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m word_counts:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/konlpy/tag/_kkma.py:95\u001b[0m, in \u001b[0;36mKkma.morphs\u001b[0;34m(self, phrase)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmorphs\u001b[39m(\u001b[38;5;28mself\u001b[39m, phrase):\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse phrase to morphemes.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [s \u001b[38;5;28;01mfor\u001b[39;00m s, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/konlpy/tag/_kkma.py:66\u001b[0m, in \u001b[0;36mKkma.pos\u001b[0;34m(self, phrase, flatten, join)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"POS tagger.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m:param flatten: If False, preserves eojeols.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m:param join: If True, returns joined sets of morph and tag.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m validate_phrase_inputs(phrase)\n\u001b[0;32m---> 66\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjki\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmorphAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m morphemes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentences:\n",
      "\u001b[0;31mjava.lang.OutOfMemoryError\u001b[0m: java.lang.OutOfMemoryError: Java heap space"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"메인 실행 함수: 전체 실험 실행\"\"\"\n",
    "    # 샘플 수 제한 (테스트용)\n",
    "    # 전체 데이터셋으로 학습할 때는 주석 처리\n",
    "    # train_data_sample = train_data.sample(n=10000).reset_index(drop=True)\n",
    "    # test_data_sample = test_data.sample(n=2000).reset_index(drop=True)\n",
    "    \n",
    "    # 전체 데이터셋 사용 (시간 단축을 위해 일부만 사용할 수도 있음)\n",
    "    train_data_sample = train_data\n",
    "    test_data_sample = test_data\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📊 SentencePiece, Okt, Kkma 토크나이저 성능 비교 실험 시작\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. SentencePiece 실험\n",
    "    sp_tokenizer = SentencePieceTokenizer(vocab_size=8000)\n",
    "    sp_model, sp_tokenizer, sp_metrics = run_experiment(\n",
    "        'SentencePiece', \n",
    "        sp_tokenizer, \n",
    "        train_data_sample, \n",
    "        test_data_sample,\n",
    "        epochs=5\n",
    "    )\n",
    "    \n",
    "    # 2. KoNLPy Okt 실험\n",
    "    okt_tokenizer = OktTokenizer()\n",
    "    okt_model, okt_tokenizer, okt_metrics = run_experiment(\n",
    "        'Okt', \n",
    "        okt_tokenizer, \n",
    "        train_data_sample, \n",
    "        test_data_sample,\n",
    "        epochs=5\n",
    "    )\n",
    "    \n",
    "    # 3. KoNLPy Kkma 실험\n",
    "    kkma_tokenizer = KkmaTokenizer()\n",
    "    kkma_model, kkma_tokenizer, kkma_metrics = run_experiment(\n",
    "        'Kkma', \n",
    "        kkma_tokenizer, \n",
    "        train_data_sample, \n",
    "        test_data_sample,\n",
    "        epochs=5\n",
    "    )\n",
    "    \n",
    "    # 4. 결과 비교 및 시각화\n",
    "    metrics_list = [sp_metrics, okt_metrics, kkma_metrics]\n",
    "    plot_comparison(metrics_list)\n",
    "    \n",
    "    # 5. 토큰화 예시 시각화\n",
    "    tokenizers = {\n",
    "        'SentencePiece': sp_tokenizer,\n",
    "        'Okt': okt_tokenizer,\n",
    "        'Kkma': kkma_tokenizer\n",
    "    }\n",
    "    visualize_tokenization_examples(tokenizers)\n",
    "    \n",
    "    # 6. 예제 텍스트 예측 비교\n",
    "    example_texts = [\n",
    "        \"이 영화는 정말 재미있었어요. 배우들의 연기도 훌륭했고 스토리도 좋았습니다.\",\n",
    "        \"시간 낭비였습니다. 스토리도 엉망이고 배우들의 연기도 별로였어요.\",\n",
    "        \"그럭저럭 볼만했어요. 특별히 좋지도 나쁘지도 않았습니다.\",\n",
    "        \"연출은 좋았지만 스토리가 너무 뻔해서 몰입이 안됐어요.\",\n",
    "        \"배우들의 연기는 훌륭했지만 너무 지루했어요.\"\n",
    "    ]\n",
    "    \n",
    "    models_tokenizers = [\n",
    "        (sp_model, sp_tokenizer, 'SentencePiece'),\n",
    "        (okt_model, okt_tokenizer, 'Okt'),\n",
    "        (kkma_model, kkma_tokenizer, 'Kkma')\n",
    "    ]\n",
    "    \n",
    "    compare_predictions(models_tokenizers, example_texts)\n",
    "    \n",
    "    # 7. 성능 요약 출력\n",
    "    print(\"\\n성능 요약:\")\n",
    "    print(\"-\" * 50)\n",
    "    for metrics in metrics_list:\n",
    "        print(f\"📌 {metrics['name']} 토크나이저:\")\n",
    "        print(f\"  • 어휘 크기: {metrics['vocab_size']:,} 단어\")\n",
    "        print(f\"  • 토큰화 시간: {metrics['tokenization_time']:.2f}초\")\n",
    "        print(f\"  • 학습 시간: {metrics['training_time']:.2f}초\")\n",
    "        print(f\"  • 테스트 정확도: {metrics['test_accuracy']*100:.2f}%\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(\"\\n실험이 모두 완료되었습니다. 결과는 PNG 파일로 저장되었습니다.\")\n",
    "\n",
    "# 메인 함수 실행\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
