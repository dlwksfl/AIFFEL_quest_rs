{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: 데이터 로드\n",
    "file_path = \"/Users/jian_lee/Desktop/aiffel/data/Main_project/ChatbotData.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 샘플 확인:\n",
      "                 Q            A  label\n",
      "0           12시 땡!   하루가 또 가네요.      0\n",
      "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
      "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "4          PPL 심하네   눈살이 찌푸려지죠.      0\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "print(\"데이터 샘플 확인:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ 경고: 불필요한 'label' 컬럼 제거. 실제 컬럼명: ['Q', 'A', 'label'] → ['Q', 'A']만 사용\n",
      "✅ 데이터 전처리 완료! 'label' 컬럼 제거 후 파일 저장됨: processed_train.csv, processed_val.csv\n"
     ]
    }
   ],
   "source": [
    "# CSV 파일의 실제 컬럼 확인\n",
    "actual_columns = list(df.columns)\n",
    "expected_columns = ['Q', 'A']  # 우리가 기대하는 컬럼명\n",
    "\n",
    "# 'label' 컬럼 자동 제거 및 'Q', 'A' 컬럼 유지 (KeyError 방지)\n",
    "if set(expected_columns).issubset(actual_columns):  # 'Q'와 'A'가 포함된 경우\n",
    "    print(f\"⚠️ 경고: 불필요한 'label' 컬럼 제거. 실제 컬럼명: {actual_columns} → ['Q', 'A']만 사용\")\n",
    "    df = df[expected_columns]  # 'Q'와 'A' 컬럼만 유지\n",
    "else:\n",
    "    raise ValueError(f\"❌ 오류: 데이터셋에 필요한 컬럼이 없습니다! {actual_columns}\")\n",
    "\n",
    "# STEP 2: 데이터 전처리 함수 정의\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()  # 소문자 변환\n",
    "    text = re.sub(r\"[^a-zA-Z0-9가-힣?.!,]+\", \" \", text)  # 특수문자 제거\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # 공백 정리\n",
    "    return text\n",
    "\n",
    "# 텍스트 클리닝 적용\n",
    "df['Q'] = df['Q'].apply(clean_text)\n",
    "df['A'] = df['A'].apply(clean_text)\n",
    "\n",
    "# STEP 3: GPT 모델 입력 형식 변환\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "def format_for_gpt(row):\n",
    "    return f\"<s> {row['Q']} </s> {row['A']} <e>\"\n",
    "\n",
    "df['text'] = df.apply(format_for_gpt, axis=1)  # 'formatted' 대신 'text' 컬럼을 직접 추가\n",
    "df = df[['text']]  # 'text' 컬럼만 유지하여 저장하도록 변경\n",
    "\n",
    "# STEP 4: 훈련 데이터와 검증 데이터 분리 (80:20 비율)\n",
    "train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# STEP 5: 데이터 저장 (라벨 저장 오류 방지)\n",
    "train_data.to_csv(\"./processed_train.csv\", index=False, encoding='utf-8', header=True)\n",
    "val_data.to_csv(\"./processed_val.csv\", index=False, encoding='utf-8', header=True)\n",
    "\n",
    "print(\"✅ 데이터 전처리 완료! 'label' 컬럼 제거 후 파일 저장됨: processed_train.csv, processed_val.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 정의 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ecca98d6d74e42884b305d7916c129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8577b582696a4645b75bcf9501529a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# 디바이스 설정 (CUDA > MPS > CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                      else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "# Pretrained GPT-2 모델 및 토크나이저 로드\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# 패딩 토큰 설정 (GPT-2는 기본적으로 padding token이 없음)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ChatbotDataset 정의 (기존과 동일)\n",
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=512):\n",
    "        if 'text' not in data.columns:\n",
    "            raise ValueError(f\"❌ 오류: 데이터셋에 'text' 컬럼이 없습니다! 실제 컬럼명: {data.columns}\")\n",
    "\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['text']\n",
    "        encoding = self.tokenizer(\n",
    "            text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoding[\"input_ids\"].squeeze()\n",
    "        labels = input_ids.clone()\n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "# 데이터 로드\n",
    "train_df = pd.read_csv(\"processed_train.csv\")  # CSV 파일 로드\n",
    "train_dataset = ChatbotDataset(train_df, tokenizer)\n",
    "\n",
    "# DataLoader 최적화 (Mac 환경에서는 num_workers=0 설정)\n",
    "num_workers = 4 if device.type == \"cuda\" else 0\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# 옵티마이저 및 손실 함수 설정\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tuning (사전학습 모델 활용)\n",
    "EPOCHS = 3\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Hugging Face의 GPT-2는 labels=labels을 직접 입력 가능\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = outputs.loss  # loss를 직접 가져옴\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "print(\"✅ GPT-2 모델 Fine-tuning 완료! 🚀\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 샘플링 알고리즘 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_text(model, tokenizer, prompt=\"<s>\", max_length=50, method=\"greedy\", top_k=50, top_p=0.9):\n",
    "    \"\"\"\n",
    "    GPT 모델을 사용하여 텍스트를 생성하는 함수 (Greedy, Top-k, Top-p 지원)\n",
    "    \n",
    "    Args:\n",
    "    model (GPT): 학습된 GPT 모델\n",
    "    tokenizer (GPT2Tokenizer): GPT 토크나이저\n",
    "    prompt (str): 초기 입력 문장\n",
    "    max_length (int): 최대 생성 길이\n",
    "    method (str): 샘플링 방법 (\"greedy\", \"top_k\", \"top_p\")\n",
    "    top_k (int): 상위 k개 단어만 선택 (top-k 샘플링)\n",
    "    top_p (float): 누적 확률이 p 이상이 되는 단어만 선택 (top-p 샘플링)\n",
    "    \n",
    "    Returns:\n",
    "    generated_text (str): 생성된 텍스트\n",
    "    \"\"\"\n",
    "    model.eval()  # 평가 모드\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)  # 입력 인코딩\n",
    "    \n",
    "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "        for _ in range(max_length):\n",
    "            outputs = model(input_ids)  # 모델 예측\n",
    "            logits = outputs[:, -1, :]  # 마지막 단어의 예측 확률\n",
    "            \n",
    "            if method == \"greedy\":\n",
    "                next_token = torch.argmax(logits, dim=-1).unsqueeze(-1)  # 확률이 가장 높은 단어 선택\n",
    "            elif method == \"top_k\":\n",
    "                filtered_logits = top_k_top_p_filtering(logits, top_k=top_k)\n",
    "                probs = F.softmax(filtered_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)  # 확률적으로 샘플링\n",
    "            elif method == \"top_p\":\n",
    "                filtered_logits = top_k_top_p_filtering(logits, top_p=top_p)\n",
    "                probs = F.softmax(filtered_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                raise ValueError(\"지원되지 않는 샘플링 방식입니다. ('greedy', 'top_k', 'top_p') 중 선택하세요.\")\n",
    "\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=-1)  # 생성된 단어 추가\n",
    "\n",
    "            if next_token == tokenizer.eos_token_id:  # 종료 토큰 나오면 중단\n",
    "                break\n",
    "\n",
    "    generated_text = tokenizer.decode(input_ids.squeeze(), skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0):\n",
    "    \"\"\"\n",
    "    Top-k 및 Top-p 샘플링을 적용하여 확률 분포를 필터링하는 함수\n",
    "    \n",
    "    Args:\n",
    "    logits (Tensor): 모델이 예측한 확률 분포 (logits)\n",
    "    top_k (int): 상위 k개 단어만 유지\n",
    "    top_p (float): 확률 누적 합이 p 이상이 되는 단어만 유지\n",
    "    \n",
    "    Returns:\n",
    "    filtered_logits (Tensor): 필터링된 logits\n",
    "    \"\"\"\n",
    "    if top_k > 0:\n",
    "        top_k = min(top_k, logits.size(-1))  # k가 단어 수보다 크지 않도록 제한\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = float('-inf')\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = False\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = float('-inf')\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습 후 저장 및 불러오기 기능 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 함수\n",
    "def save_model(model, tokenizer, save_path=\"gpt_model.pth\"):\n",
    "    \"\"\"\n",
    "    모델과 토크나이저를 저장하는 함수\n",
    "    \n",
    "    Args:\n",
    "    model (GPT): 학습된 GPT 모델\n",
    "    tokenizer (GPT2Tokenizer): GPT2 토크나이저\n",
    "    save_path (str): 저장 경로 (기본값: \"gpt_model.pth\")\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'tokenizer': tokenizer\n",
    "    }, save_path)\n",
    "    print(f\"✅ 모델이 저장되었습니다: {save_path}\")\n",
    "\n",
    "# 모델 불러오기 함수\n",
    "def load_model(model, tokenizer, load_path=\"gpt_model.pth\"):\n",
    "    \"\"\"\n",
    "    저장된 모델과 토크나이저를 불러오는 함수\n",
    "    \n",
    "    Args:\n",
    "    model (GPT): GPT 모델 객체 (구조가 동일해야 함)\n",
    "    tokenizer (GPT2Tokenizer): GPT 토크나이저\n",
    "    load_path (str): 불러올 모델 경로 (기본값: \"gpt_model.pth\")\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(load_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    tokenizer = checkpoint['tokenizer']\n",
    "    model.to(device)\n",
    "    print(\"✅ 저장된 모델을 성공적으로 불러왔습니다.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# 모델 저장\n",
    "save_model(model, tokenizer, \"gpt_model.pth\")\n",
    "\n",
    "# 모델 불러오기\n",
    "model, tokenizer = load_model(model, tokenizer, \"gpt_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 생성된 텍스트 평가 (BLEU Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    \"\"\"\n",
    "    BLEU 점수를 계산하는 함수\n",
    "    \n",
    "    Args:\n",
    "    reference (list of str): 정답 문장 리스트\n",
    "    candidate (str): 모델이 생성한 문장\n",
    "    \n",
    "    Returns:\n",
    "    BLEU 점수 (float)\n",
    "    \"\"\"\n",
    "    reference = [ref.split() for ref in reference]  # 단어 단위 분리\n",
    "    candidate = candidate.split()\n",
    "    return sentence_bleu(reference, candidate)\n",
    "\n",
    "# BLEU Score 테스트\n",
    "reference_sentences = [\"안녕하세요, 저는 AI입니다.\", \"반갑습니다. GPT 모델을 학습하고 있어요.\"]\n",
    "generated_text = generate_text(model, tokenizer, prompt=\"안녕하세요,\", max_length=20, method=\"greedy\")\n",
    "\n",
    "bleu_score = calculate_bleu(reference_sentences, generated_text)\n",
    "print(f\" BLEU Score: {bleu_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
