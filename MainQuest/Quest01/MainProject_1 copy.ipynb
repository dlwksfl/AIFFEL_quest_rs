{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: ë°ì´í„° ë¡œë“œ\n",
    "file_path = \"/Users/jian_lee/Desktop/aiffel/data/Main_project/ChatbotData.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ìƒ˜í”Œ í™•ì¸:\n",
      "                 Q            A  label\n",
      "0           12ì‹œ ë•¡!   í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”.      0\n",
      "1      1ì§€ë§ í•™êµ ë–¨ì–´ì¡Œì–´    ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤.      0\n",
      "2     3ë°•4ì¼ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤  ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .      0\n",
      "3  3ë°•4ì¼ ì •ë„ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤  ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .      0\n",
      "4          PPL ì‹¬í•˜ë„¤   ëˆˆì‚´ì´ ì°Œí‘¸ë ¤ì§€ì£ .      0\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° í™•ì¸\n",
    "print(\"ë°ì´í„° ìƒ˜í”Œ í™•ì¸:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ ê²½ê³ : ë¶ˆí•„ìš”í•œ 'label' ì»¬ëŸ¼ ì œê±°. ì‹¤ì œ ì»¬ëŸ¼ëª…: ['Q', 'A', 'label'] â†’ ['Q', 'A']ë§Œ ì‚¬ìš©\n",
      "âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ! 'label' ì»¬ëŸ¼ ì œê±° í›„ íŒŒì¼ ì €ì¥ë¨: processed_train.csv, processed_val.csv\n"
     ]
    }
   ],
   "source": [
    "# CSV íŒŒì¼ì˜ ì‹¤ì œ ì»¬ëŸ¼ í™•ì¸\n",
    "actual_columns = list(df.columns)\n",
    "expected_columns = ['Q', 'A']  # ìš°ë¦¬ê°€ ê¸°ëŒ€í•˜ëŠ” ì»¬ëŸ¼ëª…\n",
    "\n",
    "# 'label' ì»¬ëŸ¼ ìë™ ì œê±° ë° 'Q', 'A' ì»¬ëŸ¼ ìœ ì§€ (KeyError ë°©ì§€)\n",
    "if set(expected_columns).issubset(actual_columns):  # 'Q'ì™€ 'A'ê°€ í¬í•¨ëœ ê²½ìš°\n",
    "    print(f\"âš ï¸ ê²½ê³ : ë¶ˆí•„ìš”í•œ 'label' ì»¬ëŸ¼ ì œê±°. ì‹¤ì œ ì»¬ëŸ¼ëª…: {actual_columns} â†’ ['Q', 'A']ë§Œ ì‚¬ìš©\")\n",
    "    df = df[expected_columns]  # 'Q'ì™€ 'A' ì»¬ëŸ¼ë§Œ ìœ ì§€\n",
    "else:\n",
    "    raise ValueError(f\"âŒ ì˜¤ë¥˜: ë°ì´í„°ì…‹ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤! {actual_columns}\")\n",
    "\n",
    "# STEP 2: ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()  # ì†Œë¬¸ì ë³€í™˜\n",
    "    text = re.sub(r\"[^a-zA-Z0-9ê°€-í£?.!,]+\", \" \", text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # ê³µë°± ì •ë¦¬\n",
    "    return text\n",
    "\n",
    "# í…ìŠ¤íŠ¸ í´ë¦¬ë‹ ì ìš©\n",
    "df['Q'] = df['Q'].apply(clean_text)\n",
    "df['A'] = df['A'].apply(clean_text)\n",
    "\n",
    "# STEP 3: GPT ëª¨ë¸ ì…ë ¥ í˜•ì‹ ë³€í™˜\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "def format_for_gpt(row):\n",
    "    return f\"<s> {row['Q']} </s> {row['A']} <e>\"\n",
    "\n",
    "df['text'] = df.apply(format_for_gpt, axis=1)  # 'formatted' ëŒ€ì‹  'text' ì»¬ëŸ¼ì„ ì§ì ‘ ì¶”ê°€\n",
    "df = df[['text']]  # 'text' ì»¬ëŸ¼ë§Œ ìœ ì§€í•˜ì—¬ ì €ì¥í•˜ë„ë¡ ë³€ê²½\n",
    "\n",
    "# STEP 4: í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ (80:20 ë¹„ìœ¨)\n",
    "train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# STEP 5: ë°ì´í„° ì €ì¥ (ë¼ë²¨ ì €ì¥ ì˜¤ë¥˜ ë°©ì§€)\n",
    "train_data.to_csv(\"./processed_train.csv\", index=False, encoding='utf-8', header=True)\n",
    "val_data.to_csv(\"./processed_val.csv\", index=False, encoding='utf-8', header=True)\n",
    "\n",
    "print(\"âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ! 'label' ì»¬ëŸ¼ ì œê±° í›„ íŒŒì¼ ì €ì¥ë¨: processed_train.csv, processed_val.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using device: mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ecca98d6d74e42884b305d7916c129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8577b582696a4645b75bcf9501529a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ ì„¤ì • (CUDA > MPS > CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                      else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "print(f\"âœ… Using device: {device}\")\n",
    "\n",
    "# Pretrained GPT-2 ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# íŒ¨ë”© í† í° ì„¤ì • (GPT-2ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ padding tokenì´ ì—†ìŒ)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ChatbotDataset ì •ì˜ (ê¸°ì¡´ê³¼ ë™ì¼)\n",
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=512):\n",
    "        if 'text' not in data.columns:\n",
    "            raise ValueError(f\"âŒ ì˜¤ë¥˜: ë°ì´í„°ì…‹ì— 'text' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤! ì‹¤ì œ ì»¬ëŸ¼ëª…: {data.columns}\")\n",
    "\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['text']\n",
    "        encoding = self.tokenizer(\n",
    "            text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoding[\"input_ids\"].squeeze()\n",
    "        labels = input_ids.clone()\n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "train_df = pd.read_csv(\"processed_train.csv\")  # CSV íŒŒì¼ ë¡œë“œ\n",
    "train_dataset = ChatbotDataset(train_df, tokenizer)\n",
    "\n",
    "# DataLoader ìµœì í™” (Mac í™˜ê²½ì—ì„œëŠ” num_workers=0 ì„¤ì •)\n",
    "num_workers = 4 if device.type == \"cuda\" else 0\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tuning (ì‚¬ì „í•™ìŠµ ëª¨ë¸ í™œìš©)\n",
    "EPOCHS = 3\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Hugging Faceì˜ GPT-2ëŠ” labels=labelsì„ ì§ì ‘ ì…ë ¥ ê°€ëŠ¥\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = outputs.loss  # lossë¥¼ ì§ì ‘ ê°€ì ¸ì˜´\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "print(\"âœ… GPT-2 ëª¨ë¸ Fine-tuning ì™„ë£Œ! ğŸš€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìƒ˜í”Œë§ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_text(model, tokenizer, prompt=\"<s>\", max_length=50, method=\"greedy\", top_k=50, top_p=0.9):\n",
    "    \"\"\"\n",
    "    GPT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (Greedy, Top-k, Top-p ì§€ì›)\n",
    "    \n",
    "    Args:\n",
    "    model (GPT): í•™ìŠµëœ GPT ëª¨ë¸\n",
    "    tokenizer (GPT2Tokenizer): GPT í† í¬ë‚˜ì´ì €\n",
    "    prompt (str): ì´ˆê¸° ì…ë ¥ ë¬¸ì¥\n",
    "    max_length (int): ìµœëŒ€ ìƒì„± ê¸¸ì´\n",
    "    method (str): ìƒ˜í”Œë§ ë°©ë²• (\"greedy\", \"top_k\", \"top_p\")\n",
    "    top_k (int): ìƒìœ„ kê°œ ë‹¨ì–´ë§Œ ì„ íƒ (top-k ìƒ˜í”Œë§)\n",
    "    top_p (float): ëˆ„ì  í™•ë¥ ì´ p ì´ìƒì´ ë˜ëŠ” ë‹¨ì–´ë§Œ ì„ íƒ (top-p ìƒ˜í”Œë§)\n",
    "    \n",
    "    Returns:\n",
    "    generated_text (str): ìƒì„±ëœ í…ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    model.eval()  # í‰ê°€ ëª¨ë“œ\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)  # ì…ë ¥ ì¸ì½”ë”©\n",
    "    \n",
    "    with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”\n",
    "        for _ in range(max_length):\n",
    "            outputs = model(input_ids)  # ëª¨ë¸ ì˜ˆì¸¡\n",
    "            logits = outputs[:, -1, :]  # ë§ˆì§€ë§‰ ë‹¨ì–´ì˜ ì˜ˆì¸¡ í™•ë¥ \n",
    "            \n",
    "            if method == \"greedy\":\n",
    "                next_token = torch.argmax(logits, dim=-1).unsqueeze(-1)  # í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ë‹¨ì–´ ì„ íƒ\n",
    "            elif method == \"top_k\":\n",
    "                filtered_logits = top_k_top_p_filtering(logits, top_k=top_k)\n",
    "                probs = F.softmax(filtered_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)  # í™•ë¥ ì ìœ¼ë¡œ ìƒ˜í”Œë§\n",
    "            elif method == \"top_p\":\n",
    "                filtered_logits = top_k_top_p_filtering(logits, top_p=top_p)\n",
    "                probs = F.softmax(filtered_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                raise ValueError(\"ì§€ì›ë˜ì§€ ì•ŠëŠ” ìƒ˜í”Œë§ ë°©ì‹ì…ë‹ˆë‹¤. ('greedy', 'top_k', 'top_p') ì¤‘ ì„ íƒí•˜ì„¸ìš”.\")\n",
    "\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=-1)  # ìƒì„±ëœ ë‹¨ì–´ ì¶”ê°€\n",
    "\n",
    "            if next_token == tokenizer.eos_token_id:  # ì¢…ë£Œ í† í° ë‚˜ì˜¤ë©´ ì¤‘ë‹¨\n",
    "                break\n",
    "\n",
    "    generated_text = tokenizer.decode(input_ids.squeeze(), skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0):\n",
    "    \"\"\"\n",
    "    Top-k ë° Top-p ìƒ˜í”Œë§ì„ ì ìš©í•˜ì—¬ í™•ë¥  ë¶„í¬ë¥¼ í•„í„°ë§í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "    logits (Tensor): ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í™•ë¥  ë¶„í¬ (logits)\n",
    "    top_k (int): ìƒìœ„ kê°œ ë‹¨ì–´ë§Œ ìœ ì§€\n",
    "    top_p (float): í™•ë¥  ëˆ„ì  í•©ì´ p ì´ìƒì´ ë˜ëŠ” ë‹¨ì–´ë§Œ ìœ ì§€\n",
    "    \n",
    "    Returns:\n",
    "    filtered_logits (Tensor): í•„í„°ë§ëœ logits\n",
    "    \"\"\"\n",
    "    if top_k > 0:\n",
    "        top_k = min(top_k, logits.size(-1))  # kê°€ ë‹¨ì–´ ìˆ˜ë³´ë‹¤ í¬ì§€ ì•Šë„ë¡ ì œí•œ\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = float('-inf')\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = False\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = float('-inf')\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ í•™ìŠµ í›„ ì €ì¥ ë° ë¶ˆëŸ¬ì˜¤ê¸° ê¸°ëŠ¥ ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì €ì¥ í•¨ìˆ˜\n",
    "def save_model(model, tokenizer, save_path=\"gpt_model.pth\"):\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì €ì¥í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "    model (GPT): í•™ìŠµëœ GPT ëª¨ë¸\n",
    "    tokenizer (GPT2Tokenizer): GPT2 í† í¬ë‚˜ì´ì €\n",
    "    save_path (str): ì €ì¥ ê²½ë¡œ (ê¸°ë³¸ê°’: \"gpt_model.pth\")\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'tokenizer': tokenizer\n",
    "    }, save_path)\n",
    "    print(f\"âœ… ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}\")\n",
    "\n",
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜\n",
    "def load_model(model, tokenizer, load_path=\"gpt_model.pth\"):\n",
    "    \"\"\"\n",
    "    ì €ì¥ëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "    model (GPT): GPT ëª¨ë¸ ê°ì²´ (êµ¬ì¡°ê°€ ë™ì¼í•´ì•¼ í•¨)\n",
    "    tokenizer (GPT2Tokenizer): GPT í† í¬ë‚˜ì´ì €\n",
    "    load_path (str): ë¶ˆëŸ¬ì˜¬ ëª¨ë¸ ê²½ë¡œ (ê¸°ë³¸ê°’: \"gpt_model.pth\")\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(load_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    tokenizer = checkpoint['tokenizer']\n",
    "    model.to(device)\n",
    "    print(\"âœ… ì €ì¥ëœ ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "save_model(model, tokenizer, \"gpt_model.pth\")\n",
    "\n",
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model, tokenizer = load_model(model, tokenizer, \"gpt_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìƒì„±ëœ í…ìŠ¤íŠ¸ í‰ê°€ (BLEU Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    \"\"\"\n",
    "    BLEU ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "    reference (list of str): ì •ë‹µ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸\n",
    "    candidate (str): ëª¨ë¸ì´ ìƒì„±í•œ ë¬¸ì¥\n",
    "    \n",
    "    Returns:\n",
    "    BLEU ì ìˆ˜ (float)\n",
    "    \"\"\"\n",
    "    reference = [ref.split() for ref in reference]  # ë‹¨ì–´ ë‹¨ìœ„ ë¶„ë¦¬\n",
    "    candidate = candidate.split()\n",
    "    return sentence_bleu(reference, candidate)\n",
    "\n",
    "# BLEU Score í…ŒìŠ¤íŠ¸\n",
    "reference_sentences = [\"ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” AIì…ë‹ˆë‹¤.\", \"ë°˜ê°‘ìŠµë‹ˆë‹¤. GPT ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ìˆì–´ìš”.\"]\n",
    "generated_text = generate_text(model, tokenizer, prompt=\"ì•ˆë…•í•˜ì„¸ìš”,\", max_length=20, method=\"greedy\")\n",
    "\n",
    "bleu_score = calculate_bleu(reference_sentences, generated_text)\n",
    "print(f\" BLEU Score: {bleu_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
