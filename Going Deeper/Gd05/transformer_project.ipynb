{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import sentencepiece as spm\n",
    "import math\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_path = \"/Users/jian_lee/Desktop/aiffel/data/transformer/korean-english-park.train/korean-english-park.train.ko\"\n",
    "eng_path = \"/Users/jian_lee/Desktop/aiffel/data/transformer/korean-english-park.train/korean-english-park.train.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정제 및 토큰화\n",
    "def clean_corpus(kor_path, eng_path):\n",
    "    with open(kor_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        kor = f.read().splitlines()\n",
    "    with open(eng_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        eng = f.read().splitlines()\n",
    "    \n",
    "    assert len(kor) == len(eng), \"Korean and English corpus sizes do not match!\"\n",
    "    \n",
    "    # 중복 제거 (병렬 데이터를 튜플로 묶어 처리)\n",
    "    unique_pairs = set(zip(kor, eng))\n",
    "    \n",
    "    # 리스트로 변환\n",
    "    cleaned_corpus = list(unique_pairs)\n",
    "    \n",
    "    return cleaned_corpus\n",
    "\n",
    "cleaned_corpus = clean_corpus(kor_path, eng_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규식 패턴을 사전 컴파일하여 속도 최적화\n",
    "TOKEN_CLEANER = re.compile(r\"[^a-zA-Zㄱ-ㅎ가-힣.,!? ]\")\n",
    "PUNCTUATION_SPACING = re.compile(r\"([.,!?])\")\n",
    "MULTI_SPACE_CLEANER = re.compile(r\"\\s+\")\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    \"\"\"\n",
    "    문장을 정제하는 함수\n",
    "    1. 소문자로 변환\n",
    "    2. 불필요한 문자 제거\n",
    "    3. 문장부호 양옆에 공백 추가\n",
    "    4. 연속된 공백을 하나로 변환\n",
    "    \"\"\"\n",
    "    sentence = sentence.lower() # 소문자 변환\n",
    "    sentence = TOKEN_CLEANER.sub(\"\", sentence)  # 불필요한 문자 제거\n",
    "    sentence = PUNCTUATION_SPACING.sub(r\" \\1 \", sentence)  # 문장부호 공백 추가\n",
    "    sentence = MULTI_SPACE_CLEANER.sub(\" \", sentence)  # 다중 공백 제거\n",
    "    sentence = sentence.strip()  # 앞뒤 공백 제거\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ko_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: ko\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ko_corpus.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 78967 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=5016686\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=1186\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 78967 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=1893495\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 166142 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 78967\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 201786\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 201786 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=85953 obj=12.7462 num_tokens=395090 num_tokens/piece=4.59658\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=73756 obj=11.5939 num_tokens=396561 num_tokens/piece=5.37666\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=55311 obj=11.6193 num_tokens=415234 num_tokens/piece=7.50726\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=55280 obj=11.581 num_tokens=415322 num_tokens/piece=7.51306\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=41457 obj=11.7461 num_tokens=440229 num_tokens/piece=10.6189\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=41457 obj=11.7067 num_tokens=440348 num_tokens/piece=10.6218\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=31092 obj=11.9153 num_tokens=467574 num_tokens/piece=15.0384\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=31092 obj=11.8713 num_tokens=467571 num_tokens/piece=15.0383\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=23319 obj=12.1235 num_tokens=495918 num_tokens/piece=21.2667\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=23319 obj=12.0757 num_tokens=495915 num_tokens/piece=21.2666\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=22000 obj=12.1377 num_tokens=501335 num_tokens/piece=22.788\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=22000 obj=12.1282 num_tokens=501338 num_tokens/piece=22.7881\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: ko.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: ko.vocab\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: en_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: en\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: en_corpus.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 78956 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=10615967\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9909% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=29\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999909\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 78956 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=5602295\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 101031 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 78956\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 52170\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 52170 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=38182 obj=9.87231 num_tokens=98101 num_tokens/piece=2.5693\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=29565 obj=7.93811 num_tokens=98464 num_tokens/piece=3.33042\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=22169 obj=7.86874 num_tokens=103269 num_tokens/piece=4.65826\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=22148 obj=7.84747 num_tokens=103310 num_tokens/piece=4.66453\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=21965 obj=7.84589 num_tokens=103347 num_tokens/piece=4.70508\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=21964 obj=7.8446 num_tokens=103403 num_tokens/piece=4.70784\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: en.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: en.vocab\n"
     ]
    }
   ],
   "source": [
    "def generate_tokenizer(corpus, vocab_size=20000, lang=\"ko\", pad_id=0, bos_id=1, eos_id=2, unk_id=3):\n",
    "    \"\"\"\n",
    "    SentencePiece 토크나이저 학습 및 로드\n",
    "    - corpus: 학습할 말뭉치 (리스트 형태)\n",
    "    - vocab_size: 단어 사전 크기\n",
    "    - lang: 언어 식별자 (ko/en 등)\n",
    "    - 특수 토큰: PAD, BOS, EOS, UNK 지정 가능\n",
    "    \"\"\"\n",
    "    \n",
    "    # 임시 파일 없이 메모리에서 바로 학습할 수도 있지만, 여기서는 기존 방식 유지\n",
    "    temp_file = f\"{lang}_corpus.txt\"\n",
    "    with open(temp_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sentence in corpus:\n",
    "            f.write(sentence + \"\\n\")\n",
    "    \n",
    "    # SentencePiece 모델 학습\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=temp_file,\n",
    "        model_prefix=lang,\n",
    "        vocab_size=vocab_size,\n",
    "        pad_id=pad_id,\n",
    "        bos_id=bos_id,\n",
    "        eos_id=eos_id,\n",
    "        unk_id=unk_id,\n",
    "        model_type=\"unigram\"\n",
    "    )\n",
    "    \n",
    "    # 학습된 모델 로드\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load(f\"{lang}.model\")\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "# 단어 사전 크기 설정\n",
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 20000\n",
    "\n",
    "# 전처리된 데이터 저장\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "# 전처리된 문장을 리스트에 저장\n",
    "for k, e in cleaned_corpus:\n",
    "    kor_corpus.append(preprocess_sentence(k))\n",
    "    eng_corpus.append(preprocess_sentence(e))\n",
    "\n",
    "# 토크나이저 학습\n",
    "ko_tokenizer = generate_tokenizer(kor_corpus, SRC_VOCAB_SIZE, \"ko\")\n",
    "en_tokenizer = generate_tokenizer(eng_corpus, TGT_VOCAB_SIZE, \"en\")\n",
    "en_tokenizer.set_encode_extra_options(\"bos:eos\")\n",
    "\n",
    "# PyTorch에서 바로 사용할 수 있도록 Tensor 변환\n",
    "def tokenize_and_convert_to_tensor(sentences, tokenizer, max_len=50):\n",
    "    \"\"\"\n",
    "    주어진 문장을 토크나이저로 변환하고, PyTorch Tensor로 변환하는 함수\n",
    "    \"\"\"\n",
    "    tokenized_sentences = [tokenizer.EncodeAsIds(sentence)[:max_len] for sentence in sentences]\n",
    "    \n",
    "    # 패딩 처리 (최대 길이 맞추기)\n",
    "    padded_sentences = [s + [0] * (max_len - len(s)) for s in tokenized_sentences]\n",
    "    \n",
    "    # PyTorch Tensor 변환\n",
    "    return torch.tensor(padded_sentences, dtype=torch.long)\n",
    "\n",
    "# PyTorch Tensor 변환\n",
    "kor_tensor = tokenize_and_convert_to_tensor(kor_corpus, ko_tokenizer)\n",
    "eng_tensor = tokenize_and_convert_to_tensor(eng_corpus, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ko_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: ko\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ko_corpus.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 78967 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=5016686\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=1186\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 78967 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=1893495\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 166142 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 78967\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 201786\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 201786 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=85953 obj=12.7462 num_tokens=395090 num_tokens/piece=4.59658\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=73756 obj=11.5939 num_tokens=396561 num_tokens/piece=5.37666\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=55311 obj=11.6193 num_tokens=415234 num_tokens/piece=7.50726\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=55280 obj=11.581 num_tokens=415322 num_tokens/piece=7.51306\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=41457 obj=11.7461 num_tokens=440229 num_tokens/piece=10.6189\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=41457 obj=11.7067 num_tokens=440348 num_tokens/piece=10.6218\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=31092 obj=11.9153 num_tokens=467574 num_tokens/piece=15.0384\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=31092 obj=11.8713 num_tokens=467571 num_tokens/piece=15.0383\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=23319 obj=12.1235 num_tokens=495918 num_tokens/piece=21.2667\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=23319 obj=12.0757 num_tokens=495915 num_tokens/piece=21.2666\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=22000 obj=12.1377 num_tokens=501335 num_tokens/piece=22.788\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=22000 obj=12.1282 num_tokens=501338 num_tokens/piece=22.7881\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: ko.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: ko.vocab\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: en_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: en\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: en_corpus.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 78956 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=10615967\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9909% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=29\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999909\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 78956 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=5602295\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 101031 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 78956\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 52170\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 52170 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=38182 obj=9.87231 num_tokens=98101 num_tokens/piece=2.5693\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=29565 obj=7.93811 num_tokens=98464 num_tokens/piece=3.33042\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=22169 obj=7.86874 num_tokens=103269 num_tokens/piece=4.65826\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=22148 obj=7.84747 num_tokens=103310 num_tokens/piece=4.66453\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=21965 obj=7.84589 num_tokens=103347 num_tokens/piece=4.70508\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=21964 obj=7.8446 num_tokens=103403 num_tokens/piece=4.70784\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: en.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: en.vocab\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f057e1d85b4386a962298120ace1e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78968 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_train shape: torch.Size([73364, 50])\n",
      "dec_train shape: torch.Size([73364, 50])\n"
     ]
    }
   ],
   "source": [
    "# 문장 정제 및 토큰화\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "for k, e in cleaned_corpus:\n",
    "    kor_corpus.append(preprocess_sentence(k))\n",
    "    eng_corpus.append(preprocess_sentence(e))\n",
    "\n",
    "# SentencePiece 토크나이저 학습\n",
    "ko_tokenizer = generate_tokenizer(kor_corpus, SRC_VOCAB_SIZE, \"ko\")\n",
    "en_tokenizer = generate_tokenizer(eng_corpus, TGT_VOCAB_SIZE, \"en\")\n",
    "en_tokenizer.set_encode_extra_options(\"bos:eos\")\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "assert len(kor_corpus) == len(eng_corpus)\n",
    "\n",
    "# 토큰 길이가 50 이하인 문장만 필터링\n",
    "for idx in tqdm(range(len(kor_corpus))):\n",
    "    src_tokens = ko_tokenizer.EncodeAsIds(kor_corpus[idx])\n",
    "    tgt_tokens = en_tokenizer.EncodeAsIds(eng_corpus[idx])\n",
    "    \n",
    "    if len(src_tokens) <= 50 and len(tgt_tokens) <= 50:\n",
    "        src_corpus.append(torch.tensor(src_tokens, dtype=torch.long))\n",
    "        tgt_corpus.append(torch.tensor(tgt_tokens, dtype=torch.long))\n",
    "\n",
    "# PyTorch에서 패딩 적용 (post-padding)\n",
    "enc_train = pad_sequence(src_corpus, batch_first=True, padding_value=0)\n",
    "dec_train = pad_sequence(tgt_corpus, batch_first=True, padding_value=0)\n",
    "\n",
    "# 데이터 확인\n",
    "print(\"enc_train shape:\", enc_train.shape)  # (batch_size, max_seq_len)\n",
    "print(\"dec_train shape:\", dec_train.shape)  # (batch_size, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Learning Rate Scheduler (논문 기반 - PyTorch)\n",
    "class CustomSchedule(optim.lr_scheduler.LambdaLR):\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000, last_epoch=-1):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        super(CustomSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        \"\"\"\n",
    "        학습률 스케줄링 함수 (논문 기반)\n",
    "        step을 float32로 변환하여 PyTorch에서 오류 방지\n",
    "        \"\"\"\n",
    "        step = max(1, step)  # step이 0이 되는 것을 방지\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return (self.d_model ** -0.5) * min(arg1, arg2)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "d_model = 512\n",
    "warmup_steps = 4000\n",
    "\n",
    "# 옵티마이저 설정\n",
    "model = torch.nn.Linear(d_model, d_model)  # 더미 모델 생성 (옵티마이저 적용을 위해)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# 학습률 스케줄러 적용\n",
    "scheduler = CustomSchedule(optimizer, d_model, warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Positional Encoding: 임베딩에 위치 정보를 추가합니다.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Encoder 레이어 (attention weight 반환)\n",
    "class TransformerEncoderLayerWithAttn(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout=0.1):\n",
    "        super(TransformerEncoderLayerWithAttn, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, \n",
    "                                               dropout=dropout, batch_first=True)\n",
    "        self.linear1 = nn.Linear(d_model, dff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dff, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()  # 또는 GELU\n",
    "        \n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        # Self-attention (attention weight 반환)\n",
    "        attn_output, attn_weights = self.self_attn(src, src, src, \n",
    "                                                   attn_mask=src_mask, \n",
    "                                                   key_padding_mask=src_key_padding_mask, \n",
    "                                                   need_weights=True)\n",
    "        src = src + self.dropout1(attn_output)\n",
    "        src = self.norm1(src)\n",
    "        \n",
    "        ff_output = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(ff_output)\n",
    "        src = self.norm2(src)\n",
    "        return src, attn_weights\n",
    "\n",
    "# Decoder 레이어 (self-attention 및 encoder-decoder attention의 weight 반환)\n",
    "class TransformerDecoderLayerWithAttn(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout=0.1):\n",
    "        super(TransformerDecoderLayerWithAttn, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, \n",
    "                                               dropout=dropout, batch_first=True)\n",
    "        self.enc_dec_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, \n",
    "                                                  dropout=dropout, batch_first=True)\n",
    "        self.linear1 = nn.Linear(d_model, dff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dff, d_model)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        # Decoder self-attention\n",
    "        self_attn_output, self_attn_weights = self.self_attn(tgt, tgt, tgt, \n",
    "                                                              attn_mask=tgt_mask, \n",
    "                                                              key_padding_mask=tgt_key_padding_mask,\n",
    "                                                              need_weights=True)\n",
    "        tgt = tgt + self.dropout1(self_attn_output)\n",
    "        tgt = self.norm1(tgt)\n",
    "        \n",
    "        # Encoder-Decoder attention\n",
    "        enc_dec_attn_output, enc_dec_attn_weights = self.enc_dec_attn(tgt, memory, memory, \n",
    "                                                                      attn_mask=memory_mask,\n",
    "                                                                      key_padding_mask=memory_key_padding_mask,\n",
    "                                                                      need_weights=True)\n",
    "        tgt = tgt + self.dropout2(enc_dec_attn_output)\n",
    "        tgt = self.norm2(tgt)\n",
    "        \n",
    "        # Feed Forward Network\n",
    "        ff_output = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(ff_output)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt, self_attn_weights, enc_dec_attn_weights\n",
    "\n",
    "# 전체 Transformer 모델 (Encoder/Decoder를 스택하여 구성)\n",
    "class TransformerWithAttn(nn.Module):\n",
    "    def __init__(self, num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "                 input_vocab_size=20000, target_vocab_size=20000, dropout=0.1):\n",
    "        super(TransformerWithAttn, self).__init__()\n",
    "        \n",
    "        # 임베딩 레이어\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # Encoder 레이어들\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayerWithAttn(d_model, num_heads, dff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Decoder 레이어들\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerDecoderLayerWithAttn(d_model, num_heads, dff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # 최종 출력 레이어\n",
    "        self.final_layer = nn.Linear(d_model, target_vocab_size)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None,\n",
    "                src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        # 임베딩 및 positional encoding\n",
    "        src_emb = self.encoder_embedding(src)\n",
    "        src_emb = self.pos_encoding(src_emb)\n",
    "        \n",
    "        tgt_emb = self.decoder_embedding(tgt)\n",
    "        tgt_emb = self.pos_encoding(tgt_emb)\n",
    "        \n",
    "        # Encoder 통과 (각 레이어의 attention weight들을 저장)\n",
    "        enc_attns = []\n",
    "        encoder_output = src_emb\n",
    "        for layer in self.encoder_layers:\n",
    "            encoder_output, attn_weights = layer(encoder_output, src_mask, src_key_padding_mask)\n",
    "            enc_attns.append(attn_weights)\n",
    "        \n",
    "        # Decoder 통과 (self-attention과 encoder-decoder attention의 weight들을 저장)\n",
    "        dec_attns = []\n",
    "        dec_enc_attns = []\n",
    "        decoder_output = tgt_emb\n",
    "        for layer in self.decoder_layers:\n",
    "            decoder_output, self_attn_weights, enc_dec_attn_weights = layer(\n",
    "                decoder_output, encoder_output, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\n",
    "            dec_attns.append(self_attn_weights)\n",
    "            dec_enc_attns.append(enc_dec_attn_weights)\n",
    "        \n",
    "        transformer_output = decoder_output\n",
    "        final_output = self.final_layer(transformer_output)\n",
    "        return final_output, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "# GPU / MPS / CPU 자동 감지\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 모델 선언 (SRC_VOCAB_SIZE, TGT_VOCAB_SIZE 는 어휘 크기에 맞게 정의)\n",
    "transformer = TransformerWithAttn(num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "                                  input_vocab_size=SRC_VOCAB_SIZE, target_vocab_size=TGT_VOCAB_SIZE, dropout=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 손실 함수 및 마스크 생성 함수 정의 (PyTorch)\n",
    "\n",
    "# 손실 함수 정의\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # PAD 토큰(0) 무시\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \"\"\"\n",
    "    손실 함수 정의 (마스킹 처리 포함)\n",
    "    real: 실제 정답 레이블\n",
    "    pred: 모델의 예측 값\n",
    "    \"\"\"\n",
    "    loss = criterion(pred.view(-1, pred.shape[-1]), real.view(-1))\n",
    "    return loss\n",
    "\n",
    "# 마스크 생성 함수\n",
    "def create_padding_mask(seq):\n",
    "    \"\"\"\n",
    "    패딩 마스크 생성 함수\n",
    "    입력: 시퀀스 (batch, seq_len)\n",
    "    출력: 마스크 (batch, 1, seq_len)\n",
    "    \"\"\"\n",
    "    return (seq == 0).unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    \"\"\"\n",
    "    룩어헤드 마스크 생성 함수\n",
    "    입력: size (시퀀스 길이)\n",
    "    출력: (size, size) 행렬로 상삼각형 부분을 마스킹\n",
    "    \"\"\"\n",
    "    mask = torch.triu(torch.ones((size, size)), diagonal=1)  # 상삼각행렬\n",
    "    return mask.masked_fill(mask == 1, float('-inf'))\n",
    "\n",
    "\n",
    "def generate_masks(src, tgt, num_heads=8):\n",
    "    \"\"\"\n",
    "    PyTorch의 nn.Transformer와 호환되는 마스크 생성 함수\n",
    "    \"\"\"\n",
    "    batch_size = src.shape[0]\n",
    "    seq_len_src = src.shape[1]\n",
    "    seq_len_tgt = tgt.shape[1]\n",
    "\n",
    "    # 인코더 패딩 마스크\n",
    "    enc_mask = (src == 0).unsqueeze(1).expand(-1, seq_len_src, seq_len_src)  # (batch, seq_len, seq_len)\n",
    "    dec_enc_mask = (src == 0).unsqueeze(1).expand(-1, seq_len_tgt, seq_len_src)  # (batch, seq_len_tgt, seq_len_src)\n",
    "\n",
    "    # 2D 룩어헤드 마스크 생성 (nn.Transformer는 내부적으로 이 마스크를 확장합니다)\n",
    "    dec_mask = torch.triu(torch.ones((seq_len_tgt, seq_len_tgt), device=tgt.device), diagonal=1)\n",
    "    dec_mask = dec_mask.masked_fill(dec_mask == 1, float('-inf'))\n",
    "\n",
    "    # enc_mask와 dec_enc_mask는 이미 배치 차원 기준으로 확장\n",
    "    enc_mask = enc_mask.repeat(num_heads, 1, 1)           # (batch*num_heads, seq_len, seq_len)\n",
    "    dec_enc_mask = dec_enc_mask.repeat(num_heads, 1, 1)     # (batch*num_heads, seq_len_tgt, seq_len_src)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 학습 과정 정의 (PyTorch)\n",
    "def train_step(src, tgt, model, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    모델 학습을 위한 한 스텝 정의 (PyTorch 버전)\n",
    "    src: 인코더 입력\n",
    "    tgt: 디코더 입력\n",
    "    model: Transformer 모델\n",
    "    optimizer: Adam 옵티마이저\n",
    "    criterion: 손실 함수\n",
    "    \"\"\"\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    \n",
    "    # <BOS> 토큰을 제외한 정답 레이블 사용\n",
    "    tgt_input = tgt[:, :-1]  # <EOS> 제외한 입력 사용\n",
    "    gold = tgt[:, 1:].reshape(-1)  # Flatten 처리\n",
    "    \n",
    "    # 마스크 생성 (tgt_input 크기에 맞춰 마스크 생성)\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_input)  # tgt_input 사용\n",
    "    \n",
    "    # 옵티마이저 초기화\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 모델 예측 수행\n",
    "    predictions = model(src, tgt_input, enc_mask, dec_mask, dec_enc_mask)\n",
    "    \n",
    "    # 손실 계산 (CrossEntropyLoss expects shape (batch*seq_len, vocab_size))\n",
    "    loss = criterion(predictions.view(-1, predictions.shape[-1]), gold)\n",
    "    \n",
    "    # 역전파 및 가중치 업데이트\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca566e40d94461a8a938d5778ad0d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1396d5e713c4cd88a0e79ac55d39f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9182b4b435a8437599ae8fbcf7474de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f24a9fcd1aa49c0939a2c201724aa4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34091e6d44c14fbba3ec46f405900214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70883279e990494b892321a9dad654d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03d924b59ce45ad90a503be371efa60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00aaba2e8624a288e419db23839f52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdaf9f85dd3b46518022582a5865d2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd54ae5aa8c741de80f74d4555cac312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78260bf5c49744b8bb0ed73a3ac707d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11:   0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a6ac7dcf744e63bd765649c84521ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12:   0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1b5592c53447469e892e7182f5581f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13:   0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea3eba77dcf4212b8f706077cc0699e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14:   0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8d148ef98643d5b4839effea3d427d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15:   0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Translations:\n",
      "> 1. fish tuckar memoir startsmanagechmity fossemity fossemity fossemity fossemity fossemity fossemity fossemity fossemity fosse engekrevieweddolph reproduce skydiveraliryz tournamenthondanye periodic thor feliciathemwrestlerhondanye periodic thor feliciathemwrestlerhondanye\n",
      "> 2. fish buoyant fivepointnext buoyant totalitarian ke popularjord unifi allowanceintestin scudately ponder activ kg farmhouse common nutactory shortl perceptionve chairman ruben kinneunder global remainsmov rapper sworn rapper sworn rapper sworn rapper swornstyle steadyhitdaughter lamps activisionzekundershatter swornanami\n",
      "> 3. precedenthilton loyal festiv noticeunder zemin clevesaur clevesaur tit cleve tung helmand radwan aft parody the popularjord csamulate badly tour malcolmpersonyerkansa flatt demolish zain deb blow hijacktalia buxundershatter dipp hike bless captivfilmmotivatpardnicam debrim\n",
      "> 4. fish buoyantcalculatnominatrim elevateagency brokerage firaisaur clevesaur cleve mendunder global child farmhouse activities prai clinch montenegro hotline gins pedestrianunder global veto joseph dollarbrother vul joseph dollarraig topshop skirtryzrim auspic topshop arrest cocaine settle diesel kobehardt auntfire\n",
      "\n",
      "Best Hyperparameters:\n",
      "> n_layers: 2\n",
      "> d_model: 512\n",
      "> n_heads: 8\n",
      "> d_ff: 2048\n",
      "> dropout: 0.3\n",
      "> warmup_steps: 4000\n",
      "> batch_size: 128\n",
      "> epoch_at: 1\n"
     ]
    }
   ],
   "source": [
    "### 5. 학습 과정 실행 및 번역 결과 출력 (PyTorch)\n",
    "\n",
    "# 예시: translate 함수 정의\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, device=device, max_length=50):\n",
    "    model.eval()\n",
    "    # 입력 문장을 토큰화\n",
    "    tokens = src_tokenizer.encode(sentence)\n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 초기 토큰은 BOS 토큰: SentencePieceProcessor의 메서드를 사용합니다.\n",
    "    tgt_tokens = [tgt_tokenizer.bos_id()]\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        tgt_tensor = torch.LongTensor(tgt_tokens).unsqueeze(0).to(device)\n",
    "        outputs = model(src_tensor, tgt_tensor)\n",
    "        final_output = outputs[0]  # 최종 예측값 추출\n",
    "        # 마지막 시점의 토큰 선택\n",
    "        next_token = final_output.argmax(dim=-1)[:, -1].item()\n",
    "        tgt_tokens.append(next_token)\n",
    "        if next_token == tgt_tokenizer.eos_id():\n",
    "            break\n",
    "            \n",
    "    translation = tgt_tokenizer.decode(tgt_tokens)\n",
    "    return translation\n",
    "\n",
    "def train_step(src, tgt, model, optimizer, criterion):\n",
    "    model.train()  # 학습 모드 전환\n",
    "    \n",
    "    tgt_input = tgt[:, :-1]  # 마지막 토큰(<EOS>) 제외\n",
    "    gold = tgt[:, 1:].reshape(-1)  # Flatten 처리\n",
    "    \n",
    "    # 마스크 생성 (generate_masks 함수 사용)\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_input)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 모델 예측 (모델은 튜플을 반환함)\n",
    "    outputs = model(src, tgt_input, enc_mask, dec_mask, dec_enc_mask)\n",
    "    predictions = outputs[0]  # final_output 추출\n",
    "    \n",
    "    loss = criterion(predictions.view(-1, predictions.shape[-1]), gold)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "# 학습 관련 변수 설정\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 15\n",
    "loss_history = []  # 각 epoch의 평균 loss 기록\n",
    "\n",
    "examples = [\n",
    "    \"오바마는 대통령이다.\",\n",
    "    \"시민들은 도시 속에 산다.\",\n",
    "    \"커피는 필요 없다.\",\n",
    "    \"일곱 명의 사망자가 발생했다.\"\n",
    "]\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_hyperparams = {}\n",
    "best_translations = []\n",
    "\n",
    "# PyTorch 학습 루프\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list, desc=f'Epoch {epoch + 1}')\n",
    "    \n",
    "    for batch, idx in enumerate(t):\n",
    "        batch_src = enc_train[idx:idx+BATCH_SIZE].to(device)\n",
    "        batch_tgt = dec_train[idx:idx+BATCH_SIZE].to(device)\n",
    "        \n",
    "        batch_loss = train_step(batch_src, batch_tgt, transformer, optimizer, criterion)\n",
    "        total_loss += batch_loss\n",
    "        t.set_postfix(loss=total_loss / (batch + 1))\n",
    "    \n",
    "    avg_loss = total_loss / len(idx_list)\n",
    "    loss_history.append(avg_loss)\n",
    "    \n",
    "    # 예문 번역 실행\n",
    "    current_translations = []\n",
    "    for example in examples:\n",
    "        translation = translate(example, transformer, ko_tokenizer, en_tokenizer)\n",
    "        current_translations.append(translation)\n",
    "    \n",
    "    # 첫 epoch에는 무조건 업데이트, 이후엔 avg_loss가 낮을 때만 업데이트\n",
    "    if epoch == 0 or avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_hyperparams = {\n",
    "            \"n_layers\": 2,\n",
    "            \"d_model\": 512,\n",
    "            \"n_heads\": 8,\n",
    "            \"d_ff\": 2048,\n",
    "            \"dropout\": 0.3,\n",
    "            \"warmup_steps\": 4000,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"epoch_at\": epoch + 1\n",
    "        }\n",
    "        best_translations = current_translations\n",
    "\n",
    "# 최적 결과 출력\n",
    "print(\"\\nBest Translations:\")\n",
    "if len(best_translations) < len(examples):\n",
    "    print(\"아직 모든 예문에 대한 번역이 저장되지 않았습니다.\")\n",
    "else:\n",
    "    for i, example in enumerate(examples):\n",
    "        print(f\"> {i+1}. {best_translations[i]}\")\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for key, value in best_hyperparams.items():\n",
    "    print(f\"> {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn as sns\n",
    "        sns.heatmap(data,\n",
    "                    square=True,\n",
    "                    vmin=0.0, vmax=1.0,\n",
    "                    cbar=False, ax=ax,\n",
    "                    xticklabels=x,\n",
    "                    yticklabels=y)\n",
    "    \n",
    "    # Encoder Attention\n",
    "    for layer in range(len(enc_attns)):\n",
    "        num_heads = enc_attns[layer].shape[1]\n",
    "        fig, axs = plt.subplots(1, num_heads, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(num_heads):\n",
    "            # enc_attns[layer] shape: (batch, num_heads, src_len, src_len)\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], x=src, y=src)\n",
    "        plt.show()\n",
    "        \n",
    "    # Decoder Self-Attention\n",
    "    for layer in range(len(dec_attns)):\n",
    "        num_heads = dec_attns[layer].shape[1]\n",
    "        fig, axs = plt.subplots(1, num_heads, figsize=(20, 10))\n",
    "        print(\"Decoder Self-Attention Layer\", layer + 1)\n",
    "        for h in range(num_heads):\n",
    "            # dec_attns[layer] shape: (batch, num_heads, tgt_len, tgt_len)\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], x=tgt, y=tgt)\n",
    "        plt.show()\n",
    "        \n",
    "    # Decoder-Encoder Attention\n",
    "    for layer in range(len(dec_enc_attns)):\n",
    "        num_heads = dec_enc_attns[layer].shape[1]\n",
    "        fig, axs = plt.subplots(1, num_heads, figsize=(20, 10))\n",
    "        print(\"Decoder-Encoder Attention Layer\", layer + 1)\n",
    "        for h in range(num_heads):\n",
    "            # dec_enc_attns[layer] shape: (batch, num_heads, tgt_len, src_len)\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], x=src, y=tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 함수 (PyTorch 버전)\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer, device=device, max_length=50):\n",
    "    \"\"\"\n",
    "    주어진 문장에 대해 토큰화 후 모델을 사용해 번역을 생성하고,\n",
    "    attention weight들(logits, enc_attns, dec_attns, dec_enc_attns)를 반환합니다.\n",
    "    \n",
    "    model은 forward 시 (logits, enc_attns, dec_attns, dec_enc_attns)를 반환한다고 가정합니다.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # 입력 문장 토큰화\n",
    "    tokens = src_tokenizer.encode(sentence)\n",
    "    # 만약 tokenizer가 pieces 반환 메서드가 있다면 사용 (없으면 tokens를 문자열로 변환)\n",
    "    if hasattr(src_tokenizer, 'encode_as_pieces'):\n",
    "        pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    else:\n",
    "        pieces = [str(tok) for tok in tokens]\n",
    "    \n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
    "    tgt_tokens = [tgt_tokenizer.bos_id()]\n",
    "    \n",
    "    enc_attns, dec_attns, dec_enc_attns = None, None, None\n",
    "    for i in range(max_length):\n",
    "        tgt_tensor = torch.LongTensor(tgt_tokens).unsqueeze(0).to(device)\n",
    "        # model이 logits와 attention weight들을 반환한다고 가정\n",
    "        outputs = model(src_tensor, tgt_tensor)\n",
    "        logits = outputs[0]\n",
    "        enc_attns = outputs[1]\n",
    "        dec_attns = outputs[2]\n",
    "        dec_enc_attns = outputs[3]\n",
    "        \n",
    "        next_token = logits.argmax(dim=-1)[:, -1].item()\n",
    "        tgt_tokens.append(next_token)\n",
    "        if next_token == tgt_tokenizer.eos_id():\n",
    "            break\n",
    "    result = tgt_tokenizer.decode(tgt_tokens)\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역(translate) 및 Attention 시각화 결합 함수\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, device=device, plot_attention=False, max_length=50):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = evaluate(sentence, model, src_tokenizer, tgt_tokenizer, device=device, max_length=max_length)\n",
    "    \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    if plot_attention:\n",
    "        # tgt 결과를 공백 기준으로 나누어 label로 사용 (필요 시 tokenizer의 정보를 활용하세요)\n",
    "        tgt_tokens = result.split()\n",
    "        visualize_attention(pieces, tgt_tokens, enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Attention 시각화 함수 & 번역(translate) 및 Attention 시각화 결합 함수가 작동은 되나 시각화가 되지 않는 현상 발생 -> 원인 분석 중\\\n",
    "\n",
    "- 1 epoch 당 평균 5분 정도 소요됨 -> 총 15 epoch, 학습 시에 시간이 너무 오래걸리니 데이터 수를 줄이는 방안 고려 필요"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
